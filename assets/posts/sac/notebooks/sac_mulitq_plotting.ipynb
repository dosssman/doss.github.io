{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deps\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import wandb\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting related\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"darkgrid\") # Styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration:\n",
    "## Set the WANDB entity and project name to be used\n",
    "wandb_entity_project = \"dosssman/drlforge-multiq\"\n",
    "\n",
    "# Cache dir structure\n",
    "cache_dir = wandb_entity_project.replace(\"/\",\"_\").replace(\"-\", \"_\").replace(\".\",\"_\") # Make it simple folder name\n",
    "\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "# Episode reward cached data\n",
    "all_env_cache_filename = os.path.join( cache_dir, \"all_envs.pkl\")\n",
    "all_df_cache_filename = os.path.join( cache_dir, \"all_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Info: No cache found. Downloading data ...\n",
      "# Info: Data loaded and cached.\n"
     ]
    }
   ],
   "source": [
    "# DDPG: Data loading for Train Episode Return Plots.\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(wandb_entity_project)\n",
    "summary_list = [] \n",
    "config_list = []\n",
    "name_list = []\n",
    "envs = {}\n",
    "data = []\n",
    "rolling_average = 10\n",
    "sample_points = 500\n",
    "\n",
    "if not path.exists(all_env_cache_filename) or not path.exists(all_df_cache_filename):\n",
    "    print( \"# Info: No cache found. Downloading data ...\")\n",
    "    # Loading full data in case no cached is found\n",
    "    for idx, run in enumerate(runs):\n",
    "        ls = run.history(keys=['eval/train_episode_return', 'global_step'], pandas=False)\n",
    "        metrics_dataframe = pd.DataFrame(ls[0])\n",
    "        metrics_dataframe.insert(len(metrics_dataframe.columns), \"algo\", run.config['exp_name'])\n",
    "        metrics_dataframe.insert(len(metrics_dataframe.columns), \"seed\", run.config['seed'])\n",
    "        data += [metrics_dataframe]\n",
    "        if run.config[\"env_id\"] not in envs:\n",
    "            envs[run.config[\"env_id\"]] = [metrics_dataframe]\n",
    "            envs[run.config[\"env_id\"]+\"total_steps\"] = run.config[\"total_steps\"]\n",
    "        else:\n",
    "            envs[run.config[\"env_id\"]] += [metrics_dataframe]\n",
    "\n",
    "        # run.summary are the output key/values like accuracy.  We call ._json_dict to omit large files \n",
    "        summary_list.append(run.summary._json_dict) \n",
    "\n",
    "        # run.config is the input metrics.  We remove special values that start with _.\n",
    "        config_list.append({k:v for k,v in run.config.items() if not k.startswith('_')}) \n",
    "\n",
    "        # run.name is the name of the run.\n",
    "        name_list.append(run.name)       \n",
    "\n",
    "    summary_df = pd.DataFrame.from_records(summary_list)\n",
    "    config_df = pd.DataFrame.from_records(config_list) \n",
    "    name_df = pd.DataFrame({'name': name_list}) \n",
    "    all_df = pd.concat([name_df, config_df,summary_df], axis=1)\n",
    "    data = pd.concat(data, ignore_index=True)\n",
    "    \n",
    "    # Smoothing\n",
    "    rolling_average = 20\n",
    "    for env in envs:\n",
    "        if not env.endswith(\"total_steps\"):\n",
    "            for idx, metrics_dataframe in enumerate(envs[env]):\n",
    "                envs[env][idx] = metrics_dataframe.dropna(subset=[\"eval/train_episode_return\"])\n",
    "                envs[env][idx][\"eval/train_episode_return\"] = metrics_dataframe[\"eval/train_episode_return\"].rolling(rolling_average).mean()[rolling_average:]\n",
    "    \n",
    "    with open(all_df_cache_filename, 'wb') as handle:\n",
    "        pickle.dump(all_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(all_env_cache_filename, 'wb') as handle:\n",
    "        pickle.dump(envs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print( \"# Info: Data loaded and cached.\")\n",
    "else:\n",
    "    with open(all_df_cache_filename, 'rb') as handle:\n",
    "        all_df = pickle.load(handle)\n",
    "    with open(all_env_cache_filename, 'rb') as handle:\n",
    "        envs = pickle.load(handle)\n",
    "    print( \"# Info: Data loaded from cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# INFO: Fetching / Loading data for Ant-v2\n",
      "#  Cached data not found, reading runs data from WANDB\n"
     ]
    }
   ],
   "source": [
    "# Helper: Get Dataframe of the Episode reward given the environment\n",
    "# TODO: Implement caching\n",
    "def get_df_for_env(env_id):\n",
    "    env_total_steps = envs[env_id+\"total_steps\"]\n",
    "    env_increment = env_total_steps / 500\n",
    "    envs_same_x_axis = []\n",
    "    for sampled_run in envs[env_id]:\n",
    "        df = pd.DataFrame(columns=sampled_run.columns)\n",
    "        x_axis = [i*env_increment for i in range(500-2)]\n",
    "        current_row = 0\n",
    "        for timestep in x_axis:\n",
    "            while sampled_run.iloc[current_row][\"global_step\"] < timestep:\n",
    "                current_row += 1\n",
    "                if current_row > len(sampled_run)-2:\n",
    "                    break\n",
    "            if current_row > len(sampled_run)-2:\n",
    "                break\n",
    "            temp_row = sampled_run.iloc[current_row].copy()\n",
    "            temp_row[\"global_step\"] = timestep\n",
    "            df = df.append(temp_row)\n",
    "        \n",
    "        envs_same_x_axis += [df]\n",
    "    return pd.concat(envs_same_x_axis, ignore_index=True)\\\n",
    "\n",
    "# Caching data for all environments all at once\n",
    "ALL_ENVS = list(sorted(set(all_df[\"env_id\"])))\n",
    "ALL_ENV_EPISODE_RETURN_DF = {}\n",
    "\n",
    "ep_return_cachedir = os.path.join( cache_dir, \"episode_return\")\n",
    "if not path.exists( ep_return_cachedir):\n",
    "    os.makedirs(ep_return_cachedir)\n",
    "\n",
    "for env_name in ALL_ENVS:\n",
    "    print( f\"# INFO: Fetching / Loading data for {env_name}\")\n",
    "    \n",
    "    env_ep_return_cachefile = os.path.join( ep_return_cachedir, f\"{env_name}_cache.pkl\")\n",
    "    if not path.exists(env_ep_return_cachefile):\n",
    "        print(\"#  Cached data not found, reading runs data from WANDB\")\n",
    "        # Actually process the data. Takes most of the time.\n",
    "        env_df = get_df_for_env(env_name)\n",
    "        \n",
    "        with open(env_ep_return_cachefile, 'wb') as handle:\n",
    "            pickle.dump(env_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        # Load it from cached data\n",
    "        print(\"#  Cached data found and loaded\")\n",
    "        with open(env_ep_return_cachefile, 'rb') as handle:\n",
    "            env_df = pickle.load(handle)\n",
    "    \n",
    "    # Add the data to global variable for later usage\n",
    "    ALL_ENV_EPISODE_RETURN_DF[f\"{env_name}\"] = env_df\n",
    "\n",
    "print( \"# INFO: All data loading done for episode reward data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for one environment\n",
    "env_name = \"Hopper-v2\"\n",
    "\n",
    "# Get the dataframe\n",
    "env_df = ALL_ENV_EPISODE_RETURN_DF[env_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(env_df[\"algo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameterization\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# Plot the data onto the ax object\n",
    "sns.lineplot(data=env_df,\n",
    "    x=\"global_step\",\n",
    "    y=\"eval/train_episode_return\",\n",
    "    ci='sd', ax=ax,\n",
    "    label=\"SAC Autotuned\",\n",
    "    color=\"teal\"\n",
    ")\n",
    "\n",
    "# Plot config and pretify\n",
    "ax.set_title(env_name)\n",
    "ax.set_xlabel(\"Time steps\")\n",
    "ax.set_ylabel(\"Episode return\")\n",
    "ax.ticklabel_format(style='sci', scilimits=(0,0), axis='x')\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates plots for all the environments\n",
    "for env_name in ALL_ENVS:\n",
    "    env_df = env_df = ALL_ENV_EPISODE_RETURN_DF[env_name]\n",
    "    \n",
    "    # Plot parameterization\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "    # Plot the data onto the ax object\n",
    "    sns.lineplot(data=env_df,\n",
    "        x=\"global_step\",\n",
    "        y=\"eval/train_episode_return\",\n",
    "        ci='sd', ax=ax,\n",
    "        label=\"SAC Autotuned\",\n",
    "        color=\"teal\"\n",
    "    )\n",
    "\n",
    "    # Plot config and pretify\n",
    "    ax.set_title(env_name)\n",
    "    ax.set_xlabel(\"Time steps\")\n",
    "    ax.set_ylabel(\"Episode return\")\n",
    "    ax.ticklabel_format(style='sci', scilimits=(0,0), axis='x')\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "#     fig.savefig(f\"{env_name}_EpisodeReturn_SAC_AutotunedOnly.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
