---
layout: single
title: "[WIP] Soft Actor Critic: Implementation and Experiments"
excerpt: "An attempt at expanding upon the theory and motivation behind the Soft Actor Critic algorithm for continuous and discrete action space, as well as their respective implemention details and variations, if any exist."
tags:
  - Reinforcement Learning
  - Implementation
  - Research
  - Policy Gradients
  - Entropy Maximization
  - Soft Actor Critic
toc: true
toc_sticky: true
author_profile: true
classes: wide
comments: true
---

# Introduction

The Soft Actor Critic (SAC) ([[1]](https://arxiv.org/abs/1702.08165), [[2]](https://arxiv.org/abs/1801.01290), [[3]](https://arxiv.org/abs/1803.06773), [[4]](https://arxiv.org/abs/1812.05905), [[5]](https://arxiv.org/abs/1910.07207)) built on top of the various progress made in Deep Reinforcement Learning (DRL) for continuous control.
It happens to address a few shortcomings the Deep Deterministic Policy Gradient (DDPG) ([[6]](https://arxiv.org/abs/1509.02971)) method.
Since a the SAC algorithm is deeply related with the DDPG one, being familiar with the latter should greatly facilitate understanding the SAC algorithm itself.
For the sake a completness, a similar write-up of the DDPG algorithm and some of its intricacies can be found [here](/posts/2020-07-01-ddpg-experiments/).
Being familiar with the latter should thus greatly facilitate understanding the SAC algorithm, at least from our personal experience when trying to re-implement said algorithm.
{: .text-justify}

As an outline of this post, we first attempt at introducing the theory behind SAC as intuitively as possible.
Then, we go over the various implementation details of SAC that can be found across the various publicly available implementations, then introduce our own reimplementation, based on the work flow of the [CleanRL Reinforcement Learning library](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/sac_continuous_action.py).
Additional, we present some results of training an SAC agent over some popular DRL environments, as well as some additional experiments that we thought we thought would be interesting to play around.
{: .text-justify}

**TODO: Remove this additional experiment list once their are added at the bottom of the post**
- weight sharing between the critics
- more critics
- delayed actor updates
- critic updated over different batches
- automatic entropy tuning and its relation with actor loss and such.
- training critics on different batches to achieve some sort of specialization.
- discrete SAC
- initialization schemes, layer normalization other network tweaks respective impact.

Without further ado, let us start with an overview before diving into the nitty-gritty details of the aforementioned algorithm.
{: .text-justify}

# Background

## Reinforcement Learning

<figure style="width: 320px" class="align-right">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/posts/ddpg_experiments/agentenvinteraction.png" alt="">
  <figcaption>Figure 1: Agent-environment interaction. Picture taken from <a href="http://incompleteideas.net/book/ebook/node28.html">Richard Sutton's book</a></figcaption>
</figure>
Reinforcement Learning (RL) is usually approached through the lenses of the Markov Decision Process (MDP), in the most straight-forward cases.
The MDP is defined as a five-tuple $$(S,A,P(s'\vert s, a),R(s,a),\gamma)$$, which respectively stand for the *state space*, *action space*, *transition dynamics*, *reward function* and *discount factor* (more [here](/posts/2020-07-01-ddpg-experiments/#the-mdp-framework)).
{: .text-justify}

The objective of a standard RL agent $$\pi$$ is to maximize the expected future reward, denoted here as $$J$$, which can be formally defined as:
{: .text-justify}

$$
J_\pi = \mathbb{E}_{s_t \sim P(\cdot \vert s_{t-1}, a_{t-1}), a_t \sim \pi(\cdot \vert s_t)} \left[ \sum_{t=0}^{\infty} \gamma ^ t r(s_t,a_t)\right] \quad (1)
$$

The agent is expected to achieve such goal by a trial-and-error process of interacting with the environment (Figure 1) and collecting experience (also referred to sampled trajectories, formally written as the tuple *(s,a,r,s')*).
{: .text-justify}

## Deterministic Policy Gradient
- A quick refresh of DDPG or just a direct link even.

## Theory behind the SAC algorithm

**1. Stochastic Policy**:

In contrast with the DDPG which is build around a parameterized deterministic policy, the SAC algorithm instead leverages a *parameterized stochastic policy*, which happens to be more expressive than its deterministic counterpart.
Indeed a stochastic policy first allows the agent to capture multiple near-optimal policies in the same structure.
An example of where this comes in handy could be described as follows: let us imagine the context of a agent controlling a car, that tries to go from city A to city B.
Now there might be multiple roads that lead from A to B, and since we want the agent to get there as fast as possible, we are interested in the short path.
It might happen that there exist two different roads that have the same, shortest length.
A deterministic agent would only be able to capture a single one of them (this is assuming it can find one of them), while a stochastic agent would be able to capture both of the roads.
While this is a really simplistic example, this is an invaluable property when we need the agent to be able to achieve some arbitrary goal in different ways.
{: .text-justify}

The second benefit of having a stochastic policy would be *state-depend exploration*.
Let us consider action that are sampled from a Normal distribution, which is parameterized by the *mean* and the *standard deviation*.
The *mean* action could be considered as the deterministic action the deterministic policy would take, such as in DDPG.
The *standard deviation* allows us to define a neighborhood around said the mean, such that actions that "are not necessarily the best" also get to be sampled.
{: .text-justify}

Therefore, by conditioning the *standard deviation* parameter of our Normal distribution, the agent can systematically control how much exploration (how far away from the "current" optimal action) to go in the action space.
As an intuitive example, the *standard deviation* of the agent for a state that it has encountered often enough would become lower as the agent converges to the optimal action.
On the other hand, the *standard deviation* of the agent for a state that might have not been explore well enough yet will be larger, thus making the agent behave less conservatively, and effectively exhausting the action space to build a better knowledge of that unexplored state.

An additional benefit of having a *state-dependent* exploration is that it also results in a more smoothly correlated exploration process.
Indeed, when sampling noise from a normal distribution to add it to an action, for the same state, we would always get different and uncorrelated noise values (see [[8]](https://arxiv.org/abs/1706.01905)).
Parameterizing the "noise" of exploration by condition on the state instead results in more consistent exploratory behavior for an arbitrary state.
{: .text-justify}

**TODO: Intuitive figure**

**2. Entropy Maximization**

As a direct consequence of having a stochastic policy, on top of the environment reward the agent is supposed to maximized, an additional **entropy maximization** objective is added.
Intuitively, when we have a low entropy policy, the actions it outputs when in a given state are *consistent*, or in other word, have low variance.
For example, a deterministic policy such as in DDPG has no entropy, since for a given state, the output action shall always be the same.
{: .text-justify}

A moderate to high entropy policy, on the other hand, when given the same state, would instead output a different action every time we sample from it.
The higher the *distance* between the actions, the higher the entropy.
As described in the sub-section above, this a high entropy policy implicitly incorporates some *exploratory noise*, which is also dependent on the state itself.
{: .text-justify}

The entropy maximization objective is formally expressed by adding the entropy of the policy, weighted by as scalar in the original Equation (1), thus yielding the entropy maximization objective:

$$
J_{\pi}^{\mathrm{MaxEnt}} = \mathbb{E}_{s_t \sim P(\cdot \vert s_{t-1}, a_{t-1}), a_t \sim \pi(\cdot \vert s_t)} \left[ \sum_{t=0}^{\infty} \gamma ^ t r(s_t,a_t) + \alpha \mathcal{H}\left[\pi(\cdot \vert s_t) \right] \right] \quad (2)
$$

**TODO: Also, designing the pics.**
{: .text-justify}

**3. Soft Q Value function**:

As a consequence of introducing the entropy maximization objective, the critic component of the standard actor-critic framework is also impacted.
Namely, we now use a *Soft Q value function* (originally presented back in [[1]](https://arxiv.org/abs/1702.08165))), which incorporates the entropy component of the policy while estimating the expected discounted future return.
{: .text-justify}

The Soft Q Value function is a transformation of the standard Q Value function into a policy by ... 
The author posit that the standard value function that evaluate a state-action pair and its corresponding entropy is an implicit policy.
Namely, by exponentiating the Q-value of all the actions given an arbitrary state, and regularizing it using the partition function over said state-action space, we can recover a probability distribution over the continuous action space.
{: .text-justify}

**TODO: Then insert that equatin here**

This implicit policy can then be used as a target for the actor's parameterized policy to optimize toward.
This process was formaly defined as minimizing the KL divergence between the implict policy recovered from the Soft Q Value and the actor's policy:
{: .text-justify}

Despite all this seemingly complicated process, the actual implementation differs from the DDPG algortihm by a really slight margin, since a lot of those *fancy* operation (exponentiaon of the Q value, regularization with partition function and minimizing the KL divergence between the two policies) happen to cancel out when using the appropriate implementation technique.
{: .text-justify}

**TODO: Inser the KL equation. Make sure the parameterized networks are introduced before though**.

**TODO: A more intuitive explanation ?**


**4. Reducing Overestimation Bias with multiple critic functions**

This *implementation trick* seems to have been introduced concurrently by not only the SAC algorithm, but also the Twin Delayed Deep Deterministic Policy Gradient (abbreviated as TD3 [[7]](https://arxiv.org/abs/1802.09477)).
(If we refer to the order in which the arXiv preprint were published, it might seem that the SAC [[2]](https://arxiv.org/abs/1801.01290) implements it first. However, the TD3 paper explicit investigates the problem of overestimation bias that occurs in DDPG, which suggests it is the origin of the technique of using multiple Q value function).
{: .text-justify}

Over-estimation bias can be understood as the Q function (or analog component) being overly optimistic, and consequently misrepresenting the landscape of the state-action space in which the agent's policy makes its decision.
Let us recall that in the policy gradient method, we aim at maximizing the likelihood of the actions that bring the agent the most reward, namely by following the guidance of a Q value function, for example.
But the Q value function itself is trained on trajectories data that are sampled with a sub-optimal policy.
If the latter does not happen to actually output actions that are close to the optimal ones for a given state, the Q value function is likely to attribute a higher value to those actions that are sub-optimal, to the detriment of the real, optimal actions.
Hence, we say that the Q-value function *overestimates the value of an action given an arbitrary state*.
{: .text-justify}

To mitigate this, the concept of using the minimum of at least 2 Q-values functions was introduced.
Intuitively, since the output of each respective Q-value function are bound to different, by taking the minimum of the estimated values of each action, we end up in a more conservative, and less biased approximation of its actual value.
{: .text-justify}

# Implementation

## Soft Q Value function
- Introducing the q function parameterized network for continuous action case.
- Mention that we actually use 2 of them, thus defined the two different \theta.
- Introducte the objective function, with the parameterized update equations
- **TODO** Mention the version with more q function their respective impact on the overall performance. Does it change anything ? If not, let us skip this part then.

## Stochastic Policy Gradient
- Introduce the stochastic policy as a parameterized neural network as well as its objective function

## Maximum entropy

- Entropy as a measure of uncertainty: recall the entropy definition of a Normal distrib and bridge it into the action of a policy
- Recall the stochastic policy introduced above, and reason that a low entropy policy is close to a determistic one, since only the actions around the mean will be sampled most of the time (sharp dist around the mean)
- A high entropy policy however, is close to a random one, in that action that are far from the mean of the distribution can also be sampled
- SAC aims retaining some degree of high entropy in the policy, which helps with exploration for example, and also allows to encode multiple near-optimal policies.

# Implementation details

## Multiple Q-Networks

- Some do 2 separate networks, some do it as a single network that returns two different values. In any case, the concept is the same.
- No weight sharing so far, but it might be interesting to see how it helps.

## Policy network

- Various ways of implementing the stochastic policy. (checkout Hafners Desmos post to illustrate the behavior of the parameterized distribution in SpinUp etc..)

## Training loop and logic

# Experiments

- Toy problems. Especially mountain car continuos will provide some discussion regarding the automatic entropy tuning and such
- Pybullet and Mujoco envs. Do we get similar results to publicly available benchmarks
- Discrete toy environments

# Conclusion

- Aims at providing a step by step guide to implement SAC, modular and easy to edit. Also plug in CleanRL link
- Added a few other experiments to help improve some performance. While most do not really matter, at least you know it know.

# Acknowledgment
- OpenAI Spinning Up , Harnooja, Kostrykov and Denis Yarats' respectivev implementations.
- Costa Huang and the Clean RL group **TODO: Insert plug** for collaboration during implementations and discussions
