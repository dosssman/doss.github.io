---
layout: single
title: "[WIP] Soft Actor Critic: Implementation and Experiments"
excerpt: "An attempt at expanding upon the theory and motivation behind the Soft Actor Critic algorithm for continuous and discrete action space, as well as their respective implemention details and variations, if any exist."
tags:
  - Reinforcement Learning
  - Implementation
  - Research
  - Policy Gradients
  - Entropy Maximization
  - Soft Actor Critic
toc: true
toc_sticky: true
author_profile: true
classes: wide
comments: true
---

# Introduction

The Soft Actor Critic (SAC) ([[1]](https://arxiv.org/abs/1702.08165), [[2]](https://arxiv.org/abs/1801.01290), [[3]](https://arxiv.org/abs/1803.06773), [[4]](https://arxiv.org/abs/1812.05905), [[5]](https://arxiv.org/abs/1910.07207)) is built on top of the various progress made in Deep Reinforcement Learning (DRL) for continuous control.
It happens to address a some of the shortcomings of the Deep Deterministic Policy Gradient (DDPG) ([[6]](https://arxiv.org/abs/1509.02971)) method.
Since a the SAC algorithm is strongly related to the latter, being familiar with it should greatly facilitate understanding the SAC algorithm itself.
(For the sake a completeness, a similar write-up for the DDPG algorithm and some of its intricacies can be found [here](/posts/2020-07-01-ddpg-experiments/).)
{: .text-justify}

As an outline, we first attempt at introducing the theory behind SAC in an intuitive way.
Then, we go over the various implementation details of SAC that can be found across the various publicly available implementations, then introduce our own reimplementation, based on the work flow of the [CleanRL Reinforcement Learning library](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/sac_continuous_action.py).
Additional, we present some results of training an SAC agent over some popular DRL environments, as well as some additional experiments that we thought would be interesting to play around.
{: .text-justify}

**TODO: Remove this additional experiment list once their are added at the bottom of the post**
First draft out all the experiments and their settings (environment, variable to check for the ablation, etc...)
then start queuing them into the wandb repository.
Do it maybe for the simplest envs first: Hopper-v2 and HopperBulletEnv-v0 first, then expand to more envs latter.
- automatic entropy tuning and its relation with actor loss and such.
- delayed actor updates
- initialization schemes, layer normalization other network tweaks respective impact.
- impact of different activations in the intermediate layers

Without further ado, let us start with an overview before diving into the nitty-gritty details of the aforementioned algorithm.
{: .text-justify}

# Background

## Reinforcement Learning

<figure style="width: 320px" class="align-right">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/posts/ddpg_experiments/agentenvinteraction.png" alt="">
  <figcaption>Figure 1: Agent-environment interaction. Picture taken from <a href="http://incompleteideas.net/book/ebook/node28.html">Richard Sutton's book</a></figcaption>
</figure>
Reinforcement Learning (RL) is usually approached through the lenses of the Markov Decision Process (MDP), in the most straight-forward cases.
The MDP is defined as a five-tuple $$(S,A,P(s'\vert s, a),R(s,a),\gamma)$$, which respectively stand for the *state space*, *action space*, *transition dynamics*, *reward function* and *discount factor* (more [here](/posts/2020-07-01-ddpg-experiments/#the-mdp-framework)).
{: .text-justify}

The objective of a standard RL agent $$\pi$$ is to maximize the expected future reward, denoted here as $$J$$, which can be formally defined as:
{: .text-justify}

$$
J_\pi = \mathbb{E}_{s_t \sim P(\cdot \vert s_{t-1}, a_{t-1}), a_t \sim \pi(\cdot \vert s_t)} \left[ \sum_{t=0}^{\infty} \gamma ^ t r(s_t,a_t)\right] \quad (1)
$$

The agent is expected to achieve such goal by a trial-and-error process of interacting with the environment (Figure 1) and collecting experience (also referred to sampled trajectories, formally written as the tuple *(s,a,r,s')*).
{: .text-justify}

## Deterministic Policy Gradient

From personal experience, before challenging the SAC algorithm, it might be useful to get familiar with the simpler, but shared structure of offline policy gradients methods such as the [DDPG](posts/2020-07-01-ddpg-experiments/) ([OpenAI SpinningUp's DDPG](https://spinningup.openai.com/en/latest/algorithms/ddpg.html?source=post_page---------------------------)).
This is motivated by the observation that the key differences between DDPG and SAC are:
- (1) a stochastic policy (which is also related the the entropy maximization aspect)
- (2) having two Q-networks.

## Key components of the SAC algorithm

**1. Entropy Maximization**

As a direct consequence of having a stochastic policy, an additional __reward__ derived from the **entropy maximization** objective can be added on top of the environment's reward.
Intuitively, when we have a low entropy policy, the actions it outputs in a given state are *consistent*, or in other word, have low variance.
For example, a deterministic policy such as in DDPG has an entropy of 0, since for any given state, the same action will be output by the policy network every time it is queried.
{: .text-justify}

A moderate to high entropy policy, on the other hand, when given the same state, would instead output a different action every time we sample from it.
The higher the *distance* between the actions, the higher the entropy.
As described in the sub-section above, such __high entropy__ policy implicitly incorporates some *exploratory noise*, which is also dependent on the current state.
{: .text-justify}

<figure class="one">
  <iframe src="https://www.desmos.com/calculator/pjswqbhtoo?embed" width="100%" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe>
  <figcaption>Figure ???: Peak red distribution $\text{Normal}(\mu=-2, \sigma=0.25)$ on the left is low-entropy ($\mathcal{H} = 0.297029942851$), while the flatter, blue distribution on the right $\text{Normal}(\mu=2, \sigma=1)$ has a higher entropy ($\mathcal{H} = 0.899089934179
$).
</figcaption>
</figure>

The entropy maximization objective is formally expressed by adding the entropy of the policy, weighted by the scalar $\alpha$ in the original Equation (1), thus yielding the entropy maximization objective:
{: .text-justify}

$$
J_{\pi}^{\mathrm{MaxEnt}} = \mathbb{E}_{ (s_t,a_t) \sim \rho_{\pi}} \left[ \sum_{t=0}^{\infty} \gamma ^ t r(s_t,a_t) + \alpha \mathcal{H}\left[\pi(\cdot \vert s_t) \right] \right] \quad (2)
$$

where $$\rho_{\pi}$$ represets the state-actions transitions (or $$(s,a)$$ trajectories basically) that are induced by following said policy $$\pi$$.
{: .text-justify}

**2. Stochastic Policy**:

In contrast with the DDPG, which is build on top of a deterministic policy, the SAC algorithm instead leverages a *parameterized stochastic policy*, which happens to be more expressive than its deterministic counterpart.
Namely, a stochastic policy can allow the agent to capture multiple near-optimal policies in the same structure.
An example of where this comes in handy could be described as follows: let us imagine the context of a agent controlling a car, that tries to go from city A to city B.
Now there might be multiple roads that lead from A to B, and since we want the agent to get there as fast as possible, we are interested in the short path.
It might happen that there exist two different roads that have the same, shortest length.
A deterministic agent would only be able to capture a single one of them (this is assuming it can first find it), while a stochastic agent would be able to capture both of the roads.
While this is a really simplistic example, this is an invaluable property when we need the agent to be able to achieve some arbitrary goal in different ways.
{: .text-justify}

The second benefit of having a stochastic policy would be *state-dependent exploration*.
Considering the actions being sampled from a Normal distribution parameterized by the *mean* and the *standard deviation*.
The *mean* action could be considered as the deterministic action the deterministic policy would take, such as in DDPG.
The *standard deviation* allows us to define a neighborhood around said *mean action*, such that actions that "are not necessarily the best" also get sampled from time to time.
{: .text-justify}

<figure class="one align-center">
    <a href="/assets/posts/sac/StateDependentExploration.svg"><img src="/assets/posts/sac/StateDependentExploration.svg"></a>
    <figcaption>Figure 2: Left: Uncorrelated and state independent exploration achieved by adding some randomly sampled noise to the actions (DDPG, TD3). Right: State dependent and correlated noise sampling, achieved by parameterizing both the mean and the standard deviation of a Normal distribution, from which the actions are sampled during during the training (SAC, PPO, TRPO).</figcaption>
</figure>

Therefore, by conditioning the *standard deviation* parameter of our Normal distribution on the state $s$, the agent can implicitly learn to control __how much exploration__ (how far away from the "current" optimal action) to apply while interacting with the environment.
Intuitively, the *standard deviation* of the agent for a state that it has encountered often enough would become lower as the agent converges to the optimal action.
On the other hand, the *standard deviation* of the agent for a state that was not seen much before would be high.
This in turn will make the agent behave less _conservatively_ around that state, thereby making it explore more, ideally leading it to exhaust the state-action space.
{: .text-justify}

An additional benefit of having a *state-dependent* exploration is that it also results in a more smooth, correlated exploration process.
Indeed, when sampling noise from a normal distribution to add it to an action, for the same state, we would always get different and uncorrelated noise values (see [[8]](https://arxiv.org/abs/1706.01905)).
Parameterizing the "noise" of exploration by conditioning on the state results in more consistent exploratory behavior.
{: .text-justify}

For the sake of completeness, we now formally introduce the notation of the parameterized policy, as well as the derivation of the gradients of said parameters with regards the agent's objective function.
One might however skip this part at first if he is already familiar with the DRL definition, or it he does not mind just getting an more abstract picture of the algorithm.
{: .text-justify}

The stochastic policy of the agent is denoted as $$\pi_{\theta}(a \vert s)$$, which actually represents a Normal distribution with mean $\mu_{\theta}(s)$ and standard deviation $\sigma_{\theta}(s)$ such that:
{: .text-justify}

$$
a \sim \mathcal{N}( \mu_{\theta}(s), \sigma_{\theta}(s))
$$

Overloading $$J_{\pi}$$ with the entropy maximization objective introduced in Equation (2), we aim at training an agent that maximizes said $$J_{\pi}$$.
Formally, we write:
{: .text-justify}

$$
\theta^* = \mathrm{argmax}_{\theta} \enspace J_{\pi}
$$

The optimization process itself consists in incrementally changing the parameters $\theta$ of the policy network to *maximize the probability of actions that contribute to higher reward, while minimizing the probability of those that lead to low rewards*.
The entropy maximization objective, on the other hand, will impact the policy network parameters so that the actions sampled based on an arbitrary states _differ_ as much as possible, while still achieving high rewards.
Intuitively, this is equivalent to having a policy that will produce different actions for the same state, but the variance of said actions will ultimately be bounded to an interval that still allows the agent to achieve high returns on the task.
{: .text-justify}

In any case, a slightly more practical approach on the objective of the agent is presented [here](#policys-objective-and-optimization).

**3. Soft Q Value function**:

As a consequence of introducing the entropy maximization objective, the critic component of the standard actor-critic framework is also impacted.
Namely, we now use a *Soft Q value function* (originally presented back in [[1]](https://arxiv.org/abs/1702.08165)), which incorporates the entropy component of the policy while estimating the expected discounted future return.
{: .text-justify}

The author posit that the _standard value function that evaluates a state-action pair and its corresponding entropy_ is an *implicit policy*.
Namely, by exponentiating the Q-value of all the actions given an arbitrary state, and regularizing it using the partition function over said state-action space, we can recover a probability distribution over the continuous action space.
An intuitive attempt at illustrating this concept is presented in the figure below.
{: .text-justify}

<figure class="one align-center">
    <a href="/assets/posts/sac/SoftQValueImplicitPolicy.svg"><img src="/assets/posts/sac/SoftQValueImplicitPolicy.svg"></a>
    <figcaption>Figure 3: Left: Soft Q Value for an arbitrary state s, and over all the possible continuous actions. Right: The implicit policy induced by the Soft Q Value function, which is recovered by exponentiating the Soft Q value of a state-action pair (numerator of the fraction), and regularized with the partition function (denominator of that same fraction). Intuitively, it is just a "softmax" operation performed over the whole state action space</figcaption>
</figure>


This implicit policy can then be used as a target for the actor's parameterized policy to optimize against.
This process was formally defined as minimizing the KL divergence between the implicit policy recovered from the Soft Q Value and the actor's policy:
{: .text-justify}

$$
 J_{\pi}(\theta) = \mathbb{E}_{s_t \sim \mathcal{D}} \Big[ \text{D}_{\text{KL}} \big( \pi_\theta(s_t) \vert \vert \frac{exp(Q_\phi(s_t, \cdot))}{Z_\phi(s_t)} \big) \Big]
$$
(See [[2]](https://arxiv.org/abs/1801.01290), Equatin 10).

While it might sound complicated at first, the actual implementation does not differ from the DDPG Q value function
Namely, a lot of those operation (exponentiation of the Q value, regularization with partition function and minimizing the KL divergence between the two policies) happen to _simplify_ themselves latter on.
{: .text-justify}

**4. Reducing Overestimation Bias with multiple critic functions**

This *implementation component* seems to have been introduced concurrently by not only the SAC algorithm, but also the Twin Delayed Deep Deterministic Policy Gradient (abbreviated as TD3 [[7]](https://arxiv.org/abs/1802.09477)) (at least when it comes to the continuous action space case).
(If we refer to the order in which the arXiv preprint were published, it might seem that the SAC [[2]](https://arxiv.org/abs/1801.01290) implements it first. However, the TD3 paper explicit investigates the problem of overestimation bias that occurs in DDPG, which suggests it is the origin of the technique of using multiple Q value function).
{: .text-justify}

Over-estimation bias can be understood as the Q function (or analog component) being overly optimistic, and consequently misrepresenting the landscape of the state-action space in which the agent's policy makes its decision.
Let us recall that in the policy gradient method, we aim at maximizing the likelihood of the actions that bring the agent the most reward, namely by following the guidance of the Q value function.
However, the Q value function itself is trained on trajectories data that are sampled with a sub-optimal policy.
If the latter does not actually output actions that are close to the optimal ones for a given state, the Q value function is likely to attribute a higher value to those actions that are sub-optimal, to the detriment of the optimal actions.
Hence, the Q-value function *overestimates the value of an action given an arbitrary state*.
{: .text-justify}

To mitigate this, the concept of using the minimum of at least 2 , or more generally, an ensemble of Q-values functions was introduced.
Intuitively, since the output of each respective Q-value function are bound to different, by taking the minimum of the estimated values of each action, we end up in a more conservative, and less biased approximation of its actual value.
This in turn dramatically increase the performance and learning stability of the agent, as demonstrated in [[7]](https://arxiv.org/abs/1802.09477)).
{: .text-justify}

# Implementation details

This write-up compiles various implementations methods that appear across popular reinforcement learning libraries / repositories, as well as a few of the lesser known ones, namely:

- Original implementation by Tuomas Haarnoja et al. (Tensorflow v1.4) [Link](https://github.com/haarnoja/sac)
- Stable Baselines (Tensorflow 1.XX) [Link](https://github.com/hill-a/stable-baselines/tree/master/stable_baselines/sac)
- OpenAI Spinning Up (Tensorflow 1.XX, as well as PyTorch) [Link](https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/sac)
- Denis Yarats and Ilya Kostrikov's Implementation (Pytorch) [Link](https://github.com/denisyarats/pytorch_sac)

The component that happens to have the most variance in all those implementations happens to be the policy, which we go over in the next section.
{: .text-justify}

## Policy network

Given the implementations mentioned above not only use different deep learning frameworks, but also adopt different structuration, we attempt to approach their respective implementations from a high-level (pseudo code) perspective, we a focus on the flow of the mapping from the observation received by the environment to the action distribution.
{: .text-justify}

First, an high-level overview of the various policy structure encountered is summarized in the graph below.
<figure class="one">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/posts/sac/policy_structures.png" alt="">
  <figcaption>Figure 4: An overall view of the policy network structures from the reference implementations.</figcaption>
</figure>

A more detailed, while deep learning framework agnostic overview of those different implementations' respective pseudo-code can be found [further down below (Appendices)](#policy-network-detailed-pseudo-code).

**Policy network implementation differences**
The brief summary of the core difference between those implementations is as follows:
{: .text-justify}

- Either "Joint or disjoint parameterization" of the mean $\mu$ and log standard deviation $log \sigma$.
- Clipping or shifting the range of the $log \sigma$
- L2 regularization of the policy network's weights
- Squashing of the action with the $tanh$ function, and adjusting the $log\pi(a \vert s)$ correspondingly.

While the main difference in those implementation is the way the action distributions are parameterized (especially $\mathrm{log}\sigma$), their respective behaviors end up being similar, as per the following [interactive graph that modelizes the respecitve distributions](https://www.desmos.com/calculator/4sfqhlm50s).
{: .text-justify}

<figure class="one">
  <iframe src="https://www.desmos.com/calculator/hve72attpt?embed" width="100%" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe>
  <figcaption>Figure 5: An intuitive representation of the resulting action distribution based on the policy network structure: in red, the parameterization scheme used by the original author's implementation and that of Yarats' et al; in blue the one used in OpenAI Stable Baselines and OpenAI SpinningUp. Practically speaking however, there is not much difference in the final result, as the underlying neural networks converge to similar, near-optimal policies.</figcaption>
</figure>

## Hyper-parameters

Additionally, we have taken this opportunity to summarize the various hyper-parameter values used across the repositories referenced above for a more convenient comparison.
{: .text-justify}

| Hyper parameter              |    Haarnoja et al.    |    Stable Baselines          | SpinningUp                   | Yarats et al.'s                   |
| :-------------               | :----------:          | :-----------:                | :-------------:              | :----------:                      |
| **General**                  |                       |  
| Discount $$\gamma$$          | 0.99                  | 0.99                         | 0.99                         | 0.99                              |
| Initial exploration (g)      | 1000 (a)              | 0 (`random_exploration`)     | 10000 (`start_steps`)        | 5000 (`num_seed_steps`)           |
| Tau (target update coef.)    | 0.005                 | 0.005                        | 0.005                        | 0.005                             |
| Reward scaling               | Yes (b)               | No                           | No                           | No                                |
| Buffer size                  | 1e6                   | 5e5                          | 1e6                          | 1e6                               |
| Batch size                   | 256                   | 64                           | 100                          | 1024                              |
| Seeding                      | {1,...,5}             | Random by default            | 0 by default                 | 1 by default                      |
| Entropy coefficient          | 0.0                   | Auto                         | 0.2                          | 0.1                               |
| Entropy auto-tuning          | No (c)                | Yes, see above               | No                           | Yes                               |
| Learning starts (g)          | 1000 (a)              | 100 (`learning_starts`)      | 1000 (`update_after`)        | 5000 (`num_seed_steps`)           |
| **Policy**                   |                       |
| Hidden layer dim.            | 256,256               | 64,64                        | 256,256                      | 1024,1024                         |
| `Logstd` range clip          | [-20,2] Clip (d)      | [-20,2] Clip Pass gradients  | [-20,2] Clip                 | tanh, then scale within [-5,2]    |
| Squashing                    | Yes                   | Yes                          | Yes                          | Yes                               |
| Update frequency             | 1                     | 1                            | 50 (f)                       | 1                                 |
| Learning rate                | 3e-4                  | 3e-4                         | 1e-3                         | 1e-4                              |
| Activation                   | ReLU (e)              | ReLU                         | ReLU                         | ReLU                              |
| Weight, Bias init.           | Xavier, Zeros         | Xavier, Zeros                | Torch nn.Linear's default    | Torch nn.Linear's default         |
| **Soft Q-Networks**          |                       |
| Hidden layer dim             | 256,256               | 64,64                        | 256, 256                     | 1024,1024                         |
| Learning rate                | 3e-4                  | 3e-4                         | 1e-3                         | 1e-4                              |
| Update frequency             | 1                     | 1                            | 50 (f)                       | 2                                 |
| Activation                   | ReLU (e)              | ReLU                         | ReLU                         | ReLU                              |
| Weight, Bias init.           | Xavier, Zeros         | Xavier, Zeros                | Torch nn.Linear's default    | Torch nn.Linear's default         |

Some additional notes regarding the hyper-parameters, or more general components of the implementations:
{: .text-justify}
- (a) Some hyper parameters were set according to the specific environment, see [here](https://github.com/dosssman/sac/blob/master/examples/variants.py).
- (b) Also varies depending on the environment. The method of determination does not seem to be provided, however. Probably some "human priors": [Link](https://github.com/dosssman/sac/blob/master/examples/variants.py).
- (c) Added later in another [repository](https://github.com/rail-berkeley/softlearning), more specifically [here](https://github.com/rail-berkeley/softlearning/blob/46f14436f62465a02b99f431bbcf57a7fa0fd09d/softlearning/algorithms/sac.py#L253)
- (d) While some implementations ([Haarnoja et al.](https://github.com/haarnoja/sac), [Stable Baselines](https://github.com/hill-a/stable-baselines/tree/master/stable_baselines/sac), [Yarats et al](https://github.com/denisyarats/pytorch_sac), directly clip the `logstd` to the bounding interval (`[-20, 2]`), SpinningUp first applies `tanh` then scale it to the interval `[-20,2]`.
- (e) The source codes also includes the option to use the `tanh` non-linear activation, but the difference is result does not seem to be explored.
- (f) In OpenAI SpinningUP, the network updates are not done at every time step, but in an "epoch-based" fashion. Namely, a specific amount of transitions are collected, before updating the policy and critic network. In other words, the "sample-update" regime is different. There is not much difference in the empirical result, however.
- (g) The concept of `Initial exploration` and `Learning starts` can be found across various implementations, not limited to the SAC algorithm. The former is consistently used (DQN, DDPG, etc...) and determines how many steps are sampled using a uniform or random distribution over the action space, so as to fill the agent's experience with diverse, "non-biased" trajectories.
This is done to learn an accurate enough critic function at the beginning of the training, to provide a less biased, and thus better feedback to the policy once it starts to learn.
The `Learning Starts` hyper parameter is less common, and determines after how many steps we start updating the networks.
Intuitively, this also makes sure we have enough data in the replay buffer before starting the updates.
From personal experience, this hyper-parameter hardly had mostly no impact on continuous control toy problems and Mujoco locomotions tasks.
- The original author's implementation, as well the one from OpenAI SpinUp evaluates the agent's performance using a _deterministic policy_, i.e. the mean of the action. Some argue, however, that being an entropy maximization method among other things, SAC agents should still be evaluated in stochastic mode so as to exhibit the whole range of diversity learned by the policy.
{: .text-justify}

Beside the implementation details touched upon above, the other aspects of the SAC algorithm implementation are essentially the same across repositories.
From here onward, we go over a simplified implementation of the aforementioned components with the technique that worked best during our re-implementation phase.
{: .text-justify}

# Simplified implementation

## Soft Q-Value function

The Soft Q-Value is likely the component with the less variance across the various reference implementation.
Similarly to DDPG [[6]](https://arxiv.org/abs/1509.02971), the Q value function simply maps from the concatenation of the current state and the action $s || a$ to a real value.
{: .text-justify}

Hereafter, a snippet of a simple implementation of such network using the Pytorch Deep Learning framework.
{: .text-justify}

```python
# observation_shape: the dimension of the observation vector that serves as input to the network
# action_shape: the dimension of the action vector

class SoftQNetwork(nn.Module):
    def __init__(self):
        super(SoftQNetwork, self).__init__()
        self.fc1 = nn.Linear(observation_shape+action_shape, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 1)

    def forward(self, x, a):
        x = torch.Tensor(x).to(device)
        x = torch.cat([x, a], 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

The same, but more contextualized implementation can be found in the SAC implementation of the [CleanRL library](https://github.com/vwxyzjn/cleanrl/blob/f4efe9ce4a0a98becbd02be7513fd1d49097500e/cleanrl/sac_continuous_action.py#L176).
{: .text-justify}

## Soft Q-Network objective and optimization

Slightly different from the traditional Q-network objective (DDPG, TD3, etc...), which predicts the "pure" expected discounted future return given a state, the **Soft Q-Network** incorporates the entropy of the policy into its objective, namely:
{: .text-justify}

$$
Q^{\pi}(s,a) = \mathbb{E}_{s' \sim P,a' \sim \pi} \big[ R(s,a) + \gamma (Q^\pi(s',a') + \alpha \underbrace{\mathcal{H}(\pi(\cdot \vert s'))}_{\text{policy's entropy}}) \big]
$$

(A more in-depth explanation is provided in [OpenAI SpinningUp](https://spinningup.openai.com/en/latest/algorithms/sac.html), and / or the [author's definition in Eq. 3](https://arxiv.org/abs/1812.05905))

The training objective of the Soft Q-Networks thus consists in reducing the _soft Bellman residual_ as follows:

$$
J_{Q}(\theta) = \frac{1}{2} \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim \mathcal{D}} \Big[ (Q_\theta(s_t, a_t) - y)^2 \Big] \\
\text{with} \enspace y = r(s_t, a_t) + \gamma (Q_\theta(s_{t+1},a_{t+1}) - \alpha \text{log} \pi_{\phi}(a_{t+1} \vert s_{t+1})), \\
\text{and where} \enspace a_{t+1} \sim \pi_\phi(\cdot \vert s_{t+1}),
$$

where $(s_t,a_t,r_t,s_{t+1} \sim \mathcal{D})$ represent a batch of state, action, reward and next_state sample from the agent's experience (trajectories collected by having the agent's policy $\pi_{\phi}$ interact with the environment).
{: .text-justify}

The mitigation of the over-estimation bias argued for in the original paper is thus achieved by having two (or more) independently parameterized soft Q-networks.
Be it in the SAC, or the TD3 algorithm, the number of Q-network required to do so was empirically determined to be "at least 2"
{: .text-justify}

In any case, letting our two soft Q-networks be $Q_{\theta_1}$ and $Q_{\theta_2}$ respectively parameterized by $\theta_1$ and $\theta_2$, the more explicit optimization objective can be written as follows:
{: .text-justify}

$$
J_{Q}(\theta_{i}) = \frac{1}{2} \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim \mathcal{D}} \Big[ (Q_{\theta_i}(s_t, a_t) - y)^2 \Big] \\
\text{with} \enspace y = r(s_t, a_t) + \gamma ({\color{red} \min_{\theta_{1,2}}Q_{\theta_i}(s_{t+1},a_{t+1})} - \alpha \text{log} \pi_{\phi}(a_{t+1} \vert s_{t+1}))
$$

Practically speaking, this whole optimization of the Soft Q-Networks is achieved by the following snippet of code:

```python
# sample data from agent's experience
s_obs, s_actions, s_rewards, s_next_obses, s_dones = rb.sample(args.batch_size)

with torch.no_grad():
    # get next action a_{t+1} and log pi(a_{t+1} | s_{t+1})
    next_state_actions, next_state_log_pi, _ = pg.get_action(s_next_obses, device)
    # get Q_1(s_{t+1}, a_{t+1})
    qf1_next_target = qf1_target.forward(s_next_obses, next_state_actions, device)
    qf2_next_target = qf2_target.forward(s_next_obses, next_state_actions, device)
    
    # add the minimum of the Q values and add the policy entropy bonus
    min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi

     # this is the y in the equations above.
    next_q_value = torch.Tensor(s_rewards).to(device) + \
        (1 - torch.Tensor(s_dones).to(device)) * args.gamma * (min_qf_next_target).view(-1)

# get Q_1(s_t, a_t)  and Q_2(s_t, a_t)
qf1_a_values = qf1.forward(s_obs, torch.Tensor(s_actions).to(device), device).view(-1)
qf2_a_values = qf2.forward(s_obs, torch.Tensor(s_actions).to(device), device).view(-1)

# Compute the TD error with respect to y(s,a) for the Q networks
qf1_loss = loss_fn(qf1_a_values, next_q_value) 
qf2_loss = loss_fn(qf2_a_values, next_q_value)
qf_loss = (qf1_loss + qf2_loss) / 2

# here, values_optimizer is an Adam (torch.optim.Adam) optimizer that applies stochastic gradient
values_optimizer.zero_grad()
qf_loss.backward()
values_optimizer.step()
```

For the more contextualized source code, please refer to [this link](https://github.com/vwxyzjn/cleanrl/blob/af2344d3e13c5ab993075f30b5e1fdeb7aa0da38/cleanrl/sac_continuous_action.py#L258).
{: .text-justify}

As a side experiment, the impact of the number of Q-networks was empirically investigated.
Beside re-asserting the drastic improvement of having at least 2 Q-network, thus mitigating the overestimation bias, having more than 2 Q-network did not seem to worth the cost in memory, code complexification and additional computational cost (at least not in the limited tasks that were experimented on).
For the sake of completeness, the results can be accessed [here](https://wandb.ai/dosssman/drlforge-multiq/reports/SAC-Multi-Q--VmlldzoyMjEwMDM).
{: .text-justify}

## Policy network

Finally, we present a snippet of the implementation used, which uses the "disjoint parameterization" method, but the `tanh` the scale processing applying to the `logstd` component.
{: .text-justify}

```python
# observation_shape: the dimension of the observation vector that serves as input to the network
# action_shape: the dimension of the action vector
# LOG_STD_MAX = 2
# LOG_STD_MIN = -5

class Policy(nn.Module):
    def __init__(self):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(observation_shape, 256) # Better result with slightly wider networks.
        self.fc2 = nn.Linear(256, 128)
        self.mean = nn.Linear(128, action_shape)
        self.logstd = nn.Linear(128, action_shape)
        # action rescaling
        self.action_scale = torch.FloatTensor(
            (env.action_space.high - env.action_space.low) / 2.)
        self.action_bias = torch.FloatTensor(
            (env.action_space.high + env.action_space.low) / 2.)
        self.apply(layer_init)

    def forward(self, x):
        x = torch.Tensor(x).to(device)

        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        mean = self.mean(x)
        log_std = self.logstd(x)
        # From Denis Yarats' implementation: scaling the logstd with tanh,
        # and bounding it to a reasonable interval
        log_std = torch.tanh(log_std)
        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1) 

        return mean, log_std

    def get_action(self, x):
        mean, log_std = self.forward(x)
        std = log_std.exp()
        normal = Normal(mean, std)
        x_t = normal.rsample()  # reparameterization trick (mean + std * N(0,1))
        y_t = torch.tanh(x_t)
        action = y_t * self.action_scale + self.action_bias
        log_prob = normal.log_prob(x_t)
        # Enforcing Action Bound with tanh squashing
        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) +  1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        mean = torch.tanh(mean) * self.action_scale + self.action_bias

        return action, log_prob, mean

    def to(self, device):
        self.action_scale = self.action_scale.to(device)
        self.action_bias = self.action_bias.to(device)
        return super(Policy, self).to(device)
```

Similarly to the Q-network, a more contextualized implementation can be found [here](https://github.com/vwxyzjn/cleanrl/blob/f4efe9ce4a0a98becbd02be7513fd1d49097500e/cleanrl/sac_continuous_action.py#L131).
{: .text-justify}

## Policy's objective and optimization

Recall the maximum entropy objective from Eq. 2 our agent is expected to optimize toward.
In practice, with a parameterized Gaussian policy $\pi_\phi$ as described above, the objective becomes:
{: .text-justify}

$$
\text{max}_{\phi} \, J_{\pi}(\phi) = \mathbb{E}_{s \sim \mathcal{D}} \Big[ \text{min}_{j=1,2} Q_{\theta_j}( s, a) - \alpha \text{log}\pi_{\phi}(a \vert s) \Big] \\
\text{where } a = \mu_{\phi}(s) + \epsilon \, \sigma_{\phi}(s), \text{ with } \epsilon \sim \mathcal{N}(0, 1) \text{ (reparameterization trick.)}
$$

Intuitively, we optimize the weights $\phi$ of the policy network so that it (1) outputs actions $a$ that correspond to a high Q-value $Q(s,a)$, while at the same time (2) making sure that the distribution over those actions $a$ is random enough (maximum entropy).
{: .text-justify}

[This part is quite straightforward to implement, as per the follow code snippet.](https://github.com/vwxyzjn/cleanrl/blob/af2344d3e13c5ab993075f30b5e1fdeb7aa0da38/cleanrl/sac_continuous_action.py#L279)
```python
# sample data from agent's experience
s_obs, s_actions, s_rewards, s_next_obses, s_dones = rb.sample(args.batch_size)

# get action and their log probability
pi, log_pi, _ = pg.get_action(s_obs, device)

# get Q-value for state action pairs
qf1_pi = qf1.forward(s_obs, pi, device)
qf2_pi = qf2.forward(s_obs, pi, device)
min_qf_pi = torch.min(qf1_pi, qf2_pi).view(-1)

# maximiizes Q(s,a) and -log \pi(a|s), as per Eq. above
policy_loss = ((alpha * log_pi) - min_qf_pi).mean()

# apply one gradient update step
policy_optimizer.zero_grad()
policy_loss.backward()
policy_optimizer.step()
```

[_Also, a note on having two optimizers in the SAC_](#policy-loss-a-note-on-having-separate-optimizers)

## Automatic Entropy tuning
In the objective of the policy defined above, the coefficient of the _entropy bonus_ $\alpha$ is kept fixed all across the training.
As suggested by the authors in Section 5 of their _Soft Actor Critic And Applications_([[4]](https://arxiv.org/abs/1812.05905)), the original purpose of augmenting the standard reward with the entropy of the policy is to *encourage explorations* of yet well enough explored state (thus high entropy).
Conversely, for states where the policy has already learned a near-optimal policy, it would be preferable to reduce the entropy bonus of the policy, so that it does not _lose its way by exploring, due to being encouraged to have high entropy_.
{: .text-justify}

Therefore, having a fixed value for $\alpha$ does not fit this desiderata of matching the entropy bonus with the knowledge of the policy at an arbitrary state during its training.
{: .text-justify}

To mitigate this, the authors proposed a method to dynamically adjust $\alpha$ as the policy is trained.
While the theoretical method that is used is skipped for the sake of brevity, the gist would be that they first define an optimization constraint over the reward, subject to a lower-bound on the entropy of the policy (the bonus).
The optimal value for $\alpha$ happens to be the dual of the optimization problem introduced earlier, which objective is defined as follow:

$$
\alpha^{*}_t = \text{argmin}_{\alpha_t} \mathbb{E}_{a_t \sim \pi^{*}_t} \big[ -\alpha_t \, \text{log}\pi^{*}_t(a_t \vert s_t; \alpha_t) - \alpha_t \mathcal{H} \big],
$$

where $$\mathcal(H)$$ represents the _target entropy_, or in other words, the desired lower-bound for the expected entropy of the policy over the trajectory distribution $(s_t, a_t) \sim \rho_\pi$ induced by the latter.
As a heuristic for the _target entropy_, the authors use dimension of the action space of the task.
{: .text-justify}

Once the optimization problem is formulated this way, the implementation becomes relatively straightforward:
{: .text-justify}

```python
## Before the training actually begins
# defined an initial value for (log) alpha, 
# as well as thecorreponding optimizer (Adam)
target_entropy = - torch.prod(torch.Tensor(env.action_space.shape).to(device)).item()
# instead of alpha, uses log_alpha for numerical stability
log_alpha = torch.zeros(1, requires_grad=True, device=device)
alpha = log_alpha.exp().item() # the actual value of alpha
a_optimizer = optim.Adam([log_alpha], lr=1e-3)
```

With the optimizer declared above, the `log_alpha` is then updated jointly with the policy as follows, so as to tweak the value of $\alpha$ based on the current entropy of the policy, as per the equation above.
{: .text-justify}

```python
with th.no_grad():
    _, resampled_logprobs = policy.get_actions(observation_batch)
alpha_loss = (-log_alpha * (resampled_logprobs + target_entropy)).mean()

a_optimizer.zero_grad()
alpha_loss.backward()
a_optimizer.step()

alpha = log_alpha.exp().item()
```

# Experiments

## Benchmarking on standard tasks
- Toy problems. Especially mountain car continuous will provide some discussion regarding the automatic entropy tuning and such
- Pybullet and Mujoco envs. Do we get similar results to publicly available benchmarks
- Discrete toy environments

## Automatic Entropy Tuning

To get a better understanding on how the automatic entropy tuning works, SAC agents were trained using a limited range of $$\alpha$$ values, as well as the tuned $$\alpha$$ agent were run on a limited set of Mujoco robotic control tasks.
{: .text-justify}

The experiment results themselves can be further interacted with using the following [WANDB Report](https://wandb.ai/dosssman/drlforge.sac.entropy_exp/reports/Entropy-Effect-Investigation--Vmlldzo1MDg5Nw).
{: .text-justify}

**TL;DR**:
Overall, we observed that the best value for the entropy bonus coefficient, $$\alpha$$ drastically varies depending on the task.
The method proposed to automatically tune said values results in very small values of $$\alpha$$ as training progresses.
Intuitively, the more the agents trains, the less random we want its policy to b
While this method does perform well enough over the task experimented upon, it might be better used as a starting point when applying SAC to a new environment, before performing additional fine-tuning of the $$\alpha$$ coefficient itself to achieve better performance.

The conducted analysis on this component of the implementation can be found in the [appendex](#automatic-entropy-tuning-investigation).

## Multiple Q-Networks

[Earlier in the sub-section 4.](#some-of-the-theory-behind-the-sac), we went over on how using multiple critic allowed to agent to obtain a more useful approximation of the Q-value function by using at least 2 Q-networks.
While the [TD3 paper]((https://arxiv.org/abs/1802.09477)) does mention experimenting with more than 2 Q-networks, but for the TD3 algorithm.
{: .text-justify}

The objective of this set of experiment was to also verify the impact of using more than 2 Q-networks for the SAC algorithm.
{: .text-justify}

- First, we can easily observe that using at least 2 Q-network results in a drastic improvement in performance, but also stability of the learning across all tasks.
- While using 2 Q-network does good enough results, some environment did benefit from having more. Namely, having 3 Q-Networks did seem to improve not only the final performance, but also the stability and the efficiency of the agent's learning.
This trend can be observed namely on the _Hopper-v2_, _Walker2d-v2_, _Ant-v2_, and _Humanoid-v2_ (the latter, to a certain extent).
It nevertheless incurs an additional computational cost that should be taken into account when running extensive experiments.
(Ideally, the Q-network would be run 'in parallel').
- Beyond 3 Q-networks, there is hardly any consistent improvement in performance or stability in the performance of the agent.
If anything, as the number of Q-network rises, the performance seems to saturate at sub-optimal levels.
As an example, SAC with _Number of Q-Networks_ $$\in {5,8,10}$$ performs as, if not worse than when using a single Q-network.
Hypothetically, this would be due *an overly conservative Q-value* which results in the agent not exploring much, thus converging to a sub-optimal policy.
{: .text-justify}

## Policy update delay (following TD3)

This experiment consisted in investigating the impact of the _policy update delay_ mechanism notably introduced in the [TD3 paper]((https://arxiv.org/abs/1802.09477)), and subsequently appearing in other [public implementations](https://github.com/denisyarats/pytorch_sac_ae/blob/7fa560e21c026c04bb8dcd72959ecf4e3424476c/train.py#L50).
{: .text-justify}

The default practice is to update the policy network every second update of the critic networks (the hyper paramter `--actor-update-interval=2`, code wise).
Intuitively, the aim is to update the policy by using the more accurate Q-network.
{: .text-justify}

This is implemented as the snippet below:

```python
## Policy update section: delay the updates by 'actor_update_interval',
## but compensate by also doing 'actor_update_interval' updates to the network
## global_step is the number of interaction with the environment so far
## (also coincides with the number of update of the Q-networks)
if global_step % args.actor_update_interval == 0:
    for _ in range(args.actor_update_interval):
        resampled_actions, resampled_logprobs = policy.get_actions( observation_batch)

        qf1_pi = qf1(observation_batch, resampled_actions).view(-1)
        qf2_pi = qf2(observation_batch, resampled_actions).view(-1)
        min_qf_pi = th.min(qf1_pi,qf2_pi)

        policy_loss = (alpha * resampled_logprobs - min_qf_pi).mean()

        p_optimizer.zero_grad()
        policy_loss.backward()
        p_optimizer.step()

## ... rest of the training
```

- In the experiments conducted, the final performance of the agent does not differ too much across the range of values of the hyper parameter investigated in this section, namely `actor_update_interval` $$\in {1,2,3,5,8}$$).

- However, following the plots (or experiment results) below, updating the policy network with the same frequency as the Q-networks (i.e. `actor-update-interval` = $1$) actually yielded better final performance than the _delayed update version_ with `actor-update-interval` = $2$.

- As for other values of that hyper parameters, there is no consistent improvement in the agent's performance across the environments investigated.
{: .text-justify}

The observations above are based on the experimental results summarized in the following [WANDB Report](https://wandb.ai/dosssman/drlforge.sac.pol_updt_frq/reports/Policy-update-frequency-effect--Vmlldzo0ODY2NzE).
{: .text-justify}

# Conclusion

In this post, we went over the various implementations of the SAC algorithms publicly available.
The main difference were identified to be **the the policy's network structure**, namely the parameterization of distribution over actions that constitutes the policy.
We also go over the implementation of the Soft Q-Network while providing an _intuitive_ approach to the theory it is based upon.
{: .text-justify}

Following a simplified re-implementation of said the SAC algorithm (that can be found [here](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/sac_continuous_action.py)), we conduct a few experiments on other components of the algorithms, which results can be summarized as follows:
{: .text-justify}

- **Automatically tuning the entropy bonus coefficient $$\alpha$$**: the [dynamic approximation-based method](https://arxiv.org/abs/1812.05905) used to adaptively find a value for $$\alpha$$ does perform satisfactory enough across the environments experimented upon. However, it is probably better used as a starting point when applying SAC to a new environment, before performing additional fine-tuning of $$\alpha$$ to achieve better performance.
{: .text-justify}

- **Number of Soft Q-Networks**: similarly to the findings in the [TD3 paper](https://arxiv.org/abs/1802.09477), using more than 2 soft Q-network does not help in further mitigating the over estimation bias, nor does it help obtain a higher final performance.
{: .text-justify}

- Added a few other experiments to help improve some performance. While most do not really matter, at least you know it know.

# Acknowledgment

- OpenAI Spinning Up , Harnooja, Kostrykov and Denis Yarats' respective implementations.
- Costa Huang and the Clean RL group **TODO: Insert plug** for insight sharing during implementation and discussions.

# Appendices

## Policy loss: a note on having separate optimizers
_Personal anecdote_: mainly due to being fresh to Pytorch (coming straight from TF 1.XX), a superficial understand of the SAC algorithm itself, and a small dose of compulsive obsession, I spent more time that should have been on this part.
Namely, confused by the fact that during the policy optimization, only the weights $\phi$ of the latter should be updated, while the weights $\theta$ of the Q-network were to be left as is, I initially _detached_ the `min_qf_pi`, thus cutting the flow of gradients to the policy, which thus ended up only maximizing its entropy.
Needless to say, the agent's performance was only going downhill.
{: .text-justify}

This is actually part the part of the optimization process that requires us to declare two distinct optimizers objects `policy_optimizer` and `values_optimizer`, for the policy and Q-networks respectively.
By doing so, we can safely call `policy_optimizer.step()` over the policy loss that contains the term `min_qf_pi`, thus updating _only_ the weights of the policy network.
{: .text-justify}

In retrospect, it is still possible to use a single optimizer over the weights of both the policy and the Q-networks, by explicitly deactivating the gradient computation latter when querying the values of the actions sampled with the policy network (this is heavily framework dependent).
While it should technically improve the memory efficiency and speed of the forward pass for the Q-networks, the gain is probably not really worth the trouble anyway.
For the sake of completness (and personal closure), the gist would be as follow:
{: .text-justify}

```python
# Init the policy net. and Q-nets: pg, qf1, qf2 respectively
optimizer = optim.Adam([
    {"params": list(pg.parameters()), "lr": args.policy_lr},
    {"params": list(qf1.parameters()) + list(qf2.parameters()), "lr": args.q_lr}
])

# ...
# collect samples in the environment using 'pg',
# then update the policy with a random batch of those sample
s_obs, s_actions, s_rewards, s_next_obses, s_dones = rb.sample(args.batch_size)

# disable gradients for Q-networks only
qf1_pi.requires_grad_(False), qf2_pi.requires_grad_(False)

pi, log_pi, _ = pg.get_action(s_obs, device) # get action and their log probability
qf1_pi = qf1.forward(s_obs, pi, device) # get Q-value for state action pairs
qf2_pi = qf2.forward(s_obs, pi, device)
min_qf_pi = torch.min(qf1_pi, qf2_pi).view(-1)

# this loss will have no gradient for the Q-net. weights
policy_loss = ((alpha * log_pi) - min_qf_pi).mean()

# apply one gradient update step
optimizer.zero_grad()
optimizer.backward()
optimizer.step()

# re-enable the gradient computation for the Q-nets.
# otherwise, the Q-nets. will not learn when we try to update them in the next iteration
qf1_pi.requires_grad_(True), qf2_pi.requires_grad_(True)

# and so on ...
```

## Automatic Entropy tuning investigation
Below, we analyze the result for the relatively simplest environment of the Mujoco robotic locomotion tasks suite.
{: .text-justify}

**Hopper-v2**

<figure class="one">
    <a href="/assets/posts/sac/entropy_exps/Hopper_v2_Performance_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Hopper_v2_Performance_Across_Alpha.png"></a>
    <figcaption>Episodic return of the agent during training</figcaption>
</figure>

In this task, the higher the entropy coefficient $$\alpha$$, the better (dark purple line)
While the auto-tuned entropy version achieves moderately high results for a relatively lower value of $$\alpha$$, there is still some room for improvement.
Also, the performance increase are not really consistent with the increase in the value of alpha.
{: .text-justify}

<figure class="one">
    <a href="/assets/posts/sac/entropy_exps/Hopper_v2_Policy_Loss_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Hopper_v2_Policy_Loss_Across_Alpha.png"></a>
    <figcaption>Policy loss</figcaption>
    <a href="/assets/posts/sac/entropy_exps/Hopper_v2_Policy_Entropy_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Hopper_v2_Policy_Entropy_Across_Alpha.png"></a>
    <figcaption>Policy Entropy</figcaption>
    <a href="/assets/posts/sac/entropy_exps/Hopper_v2_Autotuned_Alpha_Loss.png"><img src="/assets/posts/sac/entropy_exps/Hopper_v2_Autotuned_Alpha_Loss.png"></a>
    <figcaption>Alpha loss (automatic tuning)</figcaption>
    <a href="/assets/posts/sac/entropy_exps/Hopper_v2_Autotuned_Alpha_Value.png"><img src="/assets/posts/sac/entropy_exps/Hopper_v2_Autotuned_Alpha_Value.png"></a>
    <figcaption>Alpha automatically tuned value</figcaption>
</figure>

As per the last two figures, the automatically tuned value of $\alpha$ starts high, but steadily decreases as the policy entropy does.

**Walker2D-v2**

<figure class="one">
    <a href="/assets/posts/sac/entropy_exps/Walker2d_v2_Performance_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Walker2d_v2_Performance_Across_Alpha.png"></a>
    <figcaption>Episodic return of the agent during training</figcaption>
</figure>

On the Waker2d-v2 task, however, the lower the entropy, the better the final performance of the agent.
In this case, the automatically tuned entropy version of SAC achieves the third best result, after $\alpha = 0.1$ and $\alpha = 0.2$, respectively.
This could be attributed to the increase in complexity of this task, which can thus make _randomly acting policies_ perform worse, since they would be more likely to deviate from the learned optimum.
{: .text-justify}

<figure class="one">
    <a href="/assets/posts/sac/entropy_exps/Walker2d_v2_Policy_Entropy_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Walker2d_v2_Policy_Entropy_Across_Alpha.png"></a>
    <figcaption>Policy Entropy</figcaption>
</figure>

<figure class="half">
    <a href="/assets/posts/sac/entropy_exps/Walker2d_v2_Autotuned_Alpha_Loss.png"><img src="/assets/posts/sac/entropy_exps/Walker2d_v2_Autotuned_Alpha_Loss.png"></a>
    <a href="/assets/posts/sac/entropy_exps/Walker2d_v2_Autotuned_Alpha_Value.png"><img src="/assets/posts/sac/entropy_exps/Walker2d_v2_Autotuned_Alpha_Value.png"></a>
    <figcaption>Left: Alpha loss (automatic tuning); Right: Alpha automatically tuned value</figcaption>
</figure>

Interestingly, despite having the automatically tuned entropy maintaining a value of $\alpha$ within the range of $[0.02, 0.1]$, it stills achieve competitive results with the higher (and fixed) entropy version.
This would suggest that the **automatic entropy tuning mechanism finds a good trade-off between policy randomness (useful for exploration) and optimality** with respect to the objective (reward function).
{: .text-justify}

**HalfCheetah-v2**

<figure class="one">
    <a href="/assets/posts/sac/entropy_exps/HalfCheetah_v2_Performance_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/HalfCheetah_v2_Performance_Across_Alpha.png"></a>
    <figcaption>Episodic return of the agent during training</figcaption>
</figure>

Similarly to the _Walker2d-v2_ environment, overly random (high entropy) policies achieve worse performance overall.
Namely, the best performing agent corresponds to $$\alpha = 0.1$$, closely followed by the the SAC agent with the auto-tuned entropy.
{: .text-justify}

<figure class="half">
    <a href="/assets/posts/sac/entropy_exps/HalfCheetah_v2_Autotuned_Alpha_Loss.png"><img src="/assets/posts/sac/entropy_exps/HalfCheetah_v2_Autotuned_Alpha_Loss.png"></a>
    <a href="/assets/posts/sac/entropy_exps/HalfCheetah_v2_Autotuned_Alpha_Value.png"><img src="/assets/posts/sac/entropy_exps/HalfCheetah_v2_Autotuned_Alpha_Value.png"></a>
    <figcaption>Left: Alpha loss (automatic tuning); Right: Alpha automatically tuned value</figcaption>
</figure>

Following the plot above, the auto-tuned entropy version of SAC maintains a value of $$alpha$$ around $$0.2$$ across it lifetime.
Nevertheless, it still manages to achieve a better performance than the agent that uses a fixed value of $$0.2$$.
However, it is hard to draw any solid conclusion from this observation, with the small amount of seeds used to run the experiment.
{: .text-justify}

**Ant-v2**

Unlike the relatively simpler environments above, the auto-tuned SAC version displays a slight dip in performance during training, although it does recover by the end of the training.
A similar behavior is also observed for the SAC agent with a fixed value $\alpha = 0.1$, but with a smaller magnitude.
{: .text-justify}

The other SAC agents using different values of $\alpha$ do achieve a more stable performance during learning.
The best performance is achieved by the SAC with a fixed $\alpha$ value of $0.2$, closely followed by the auto-tuned SAC and SAC with $\alpha = 0.1$, which are seemingly tied together.

However, the higher the value of $\alpha$, the lower the final episodic return of the agent.
More specifically, values of $\alpha$ above $0.2$ seem to cripple the agent's performance.
Indeed, the higher it is, the more random the policy of the agent. 
In a task with a relatively high-dimensional action space such as __Ant-v2__ (8 degrees of freedom), a policy close to random is less likely to yield a good enough control behavior.
It is instead more beneficial to follow the optimization objective of the RL paradigm in such case.
{: .text-justify}

<figure class="one">
    <a href="/assets/posts/sac/entropy_exps/Ant_v2_Performance_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Ant_v2_Performance_Across_Alpha.png"></a>
    <figcaption>Episodic return of the agent during training</figcaption>
</figure>

To better understand those dip in performance exhibited by the auto-tuned SAC agent, we start by investigating the various training metrics.
First, we observe that the policy entropy, is relatively stable for each value of $\alpha$, as well as for the auto-tuned version.
{: .text-justify}

Consequently, the "entropy bonus" that is added the Soft Q-network value in the second equation of the Q-network optimization objective [here](#soft-q-network-objective-and-optimization) in the form of $$y = r(s_t, a_t) + \gamma (Q_\theta(s_{t+1},a_{t+1}) - \alpha \text{log} \pi_{\phi}(a_{t+1} \vert s_{t+1}))$$ should also be stable.
{: .text-justify}

<figure class="half">
    <a href="/assets/posts/sac/entropy_exps/Ant_v2_Policy_Entropy_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Ant_v2_Policy_Entropy_Across_Alpha.png"></a>
    <a href="/assets/posts/sac/entropy_exps/Ant_v2_QF_Loss_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Ant_v2_QF_Loss_Across_Alpha.png"></a>
    <figcaption>Left: Policy entropy across values of $\alpha$; Right: Soft Q-Network loss across values of $\alpha$</figcaption>
</figure>

Since the performance of an actor-critic agent is highly dependent on the quality of the critic's evaluation of the actor's action, we next investigate the loss of the soft Q-network.
{: .text-justify}

<figure class="half">
    <a href="/assets/posts/sac/entropy_exps/Ant_v2_Policy_Entropy_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Ant_v2_Policy_Entropy_Across_Alpha.png"></a>
    <a href="/assets/posts/sac/entropy_exps/Ant_v2_QF_Loss_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Ant_v2_QF_Loss_Across_Alpha.png"></a>
    <figcaption>Left: Policy entropy across values of $\alpha$; Right: Soft Q-Network loss across values of $\alpha$</figcaption>
</figure>

We observe that the Q-network loss for the auto-tuned SAC agent happens to show a large spike right around the time where its performance dips (around the 600K training step mark).
Furthermore, this is carried over to the policy loss, were the value of the policy are consequently largely overestimated, which would thus explain the worsening in performance of the agent (figures below).
As the quality of the Q-network's evaluation improves (lower loss), the policy improves correspondingly, leading to the recovery of the agent's performance.
{: .text-justify}

<figure class="half">
    <a href="/assets/posts/sac/entropy_exps/Ant_v2_Policy_Loss_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Ant_v2_Policy_Loss_Across_Alpha.png"></a>
    <a href="/assets/posts/sac/entropy_exps/Ant_v2_Autotuned_Alpha_Loss.png"><img src="/assets/posts/sac/entropy_exps/Ant_v2_Autotuned_Alpha_Loss.png"></a>
    <figcaption>Left: Policy loss across values of $\alpha$; Right: $\alpha$ loss across values of $\alpha$</figcaption>
</figure>

While the $\alpha$ loss is also impacted by the erroneous Q-network evaluation, the value of $\alpha$ does not actually change around that time frame.
Instead, is stays quite low across most of the training time, which makes it equivalent to an __entropy-less__ SAC agent (also similar to $\alpha = 0.1$).
{: .text-justify}

**Overall, we see that a small bonus in entropy results in the best performance on this environment.**
On a final note, it is still hard to say if the automatic tuning of $\alpha$ is really what causes the dip in performance during the training.
More empirical results would be desirable before drawing such conclusion.
{: .text-justify}

**Humanoid-v2**

The _Humanoid_ environment can be considered as being even more complex than the _Ant_ one.
However, the observation of _the higher the entropy of the policy, the lower the final agent's performance_ does not seem to apply in this case.
Namely, the best performing agents have a relatively high policy entropy, corresponding to values of $$\alpha \in [0.35,1]$$.
An intuitive interpretation would be that given the complexity of the environment, a high-entropy policy would be better at exploring, thus discovering optimal strategies, than a more conservative policy, for example.
{: .text-justify}

Beside that, while the agents corresponding to lower values do achieve similar performance to the group mentioned above, they seem to be less stable, as per the dips exhibited in their respective learning curve.
Moreover, the auto-tuned version of SAC happens to achieve the lowest results out of the lot.
In this case, having the lowest entropy does seem to negatively affect the final performance of the agent.
{: .text-justify}

<figure class="one">
    <a href="/assets/posts/sac/entropy_exps/Humanoid_v2_Performance_Across_Alpha.png"><img src="/assets/posts/sac/entropy_exps/Humanoid_v2_Performance_Across_Alpha.png"></a>
    <figcaption>Episodic return of the agent during training</figcaption>
</figure>

## Policy network detailed pseudo-code

This is an over simplistic, and framework agnostic attempt at describing the various policy network structure encountered while browsing through public repository.
In retrospect, the recapitulative figure should be more helpful to get the overall picture.

**1. Original implementation by Tuomas Haarnoja**

```python
# Let the observation be defined by x, with a shape of [batch_size, *observation_shape]
# Define a classifcal MLP feed forward network as the body of the policy.
x = MLP(x)

# Additionally, we a fully connected (FC) layers that outputs the mean and the logstd of the action distribution "jointly"
# Let Da be the dimension of the action space. MeanAndLogStd_FC has an output dimension of 2 * Da
means_and_logstds = MeanAndLogstd_FC(x)
action_means = means_and_logstds[:D] # The first half for the ation means, the latter for the logstds

action_logstds = means_and_logstds[D:] # The second half for the logstd
# Contrary to other implementation which separate the fully connected layers of the means and logstd, the original implementation outputs them jointly.
# There does not seem to be much difference in the results, however.

# Clipping the logstd to the range [-20., 2.]
action_logstds = clip_by_value(action_logstds, -20, 2) # This unfortunately will not produce gradients IF the logstd are outside of that range
action_stds = exp(action_logstds) # Apply exponetial to the logstd to recover the standard deviation itself,

# Instantiante the action distribution as a Gaussian parameterized by the previously obtained mean and standard deviations
action_dist = Normal( action_means, action_logstds)

# Samples some actions as well as their log probabilities
actions = action_dist.sample()
action_logprobs = action_dist.log_prob( actions)

# The actions are squashed to the [-1, 1] range by defualt with the tanh function.
# It however requires some correction to the action log probabiliities too
actions = tanh(actions)
action_logprobs = logprob_correction( action_logprobs)

# L2 regularization for the policy's weights. https://github.com/haarnoja/sac/blob/8258e33633c7e37833cc39315891e77adfbe14b2/sac/distributions/normal.py#L69

return actions, action_logprobs # The implementation actually returns determinsitcs actions (the action_means) that are used for the agent's evaluation.
```

**2. Stable Baselines**

This implementation uses a lot of "low-level" techniques to implement the various operations need to compute the action and their log probabilities
(https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/sac/policies.py).
But the concept is the same as in the original implementation.
{: .text-justify}

The main difference with the original implementation is that the gradient clipping uses a special method denoted as `clip_but_pass_gradient` instead of the default one provided by Tensorflow (or Pytorch).
It's implementations is as follows:
{: .text-justify}

```python
def clip_but_pass_gradient(input_, lower=-1., upper=1.):
    clip_up = tf.cast(input_ > upper, tf.float32)
    clip_low = tf.cast(input_ < lower, tf.float32)
    return input_ + tf.stop_gradient((upper - input_) * clip_up + (lower - input_) * clip_low)
```
Intuitively, since we have to recover the `action_stds` by exponentiating the `action_logstds`, some values of the latter can cause either gradient explosion (NaN, when logstd is too big) or gradient vanishing (namely when logstds goes to the negative infinity).
Thus clipping the values of `action_logstds` stabilizes the learning process and gives us a "nice" distribution over the actions.
Additionally, it constrains the action distribution to a reasonable standard deviation, so that we do not end up with a random policy (high standard deviations).
{: .text-justify}

The standard clamping method, however, does not pass any gradient when `logstd` happens to be outside of the "clamping range".
Therefore, there is not much feedback for the weights of the network to learn from.
The `clip_but_pass_gradient` thus seems to be used to circumvent such constraint.
{: .text-justify}

```python
# Let the observation be defined by x, with a shape of [batch_size, *observation_shape]
# Define a classifcal MLP feed forward network as the body of the policy.
x = MLP(x)

# Additionally, we have two spearate fully connected (FC) layers that output the mean and the logstd of the action distribution, respectively
action_means = ActionMeanFC(x)
action_logstds = ActionLogstdFC(x)

# Clipping the logstd to the range [-20., 2.]
action_logstds = clip_but_pass_gradient(action_logstds, -20, 2)
action_stds = exp(action_logstds) # Apply exponetial to the logstd to recover the standard deviation itself,

# Instantiate the action distribution as a Gaussian parameterized by the previously obtained mean and standard deviations
action_dist = Normal( action_means, action_logstds)

# Samples some actions as well as their log probabilities
actions = action_dist.sample()
action_logprobs = action_dist.log_prob( actions)

# At the time of writting, stable baselines did not seem to implement the L2 regularization for the means and logstd: https://github.com/hill-a/stable-baselines/blob/3d115a3f1f5a755dc92b803a1f5499c24158fb21/stable_baselines/sac/policies.py#L219

# Squash the actions to range [-1, 1] and correct the log_probs accordingly
# https://github.com/hill-a/stable-baselines/blob/3d115a3f1f5a755dc92b803a1f5499c24158fb21/stable_baselines/sac/policies.py#L44
deterministic_actions = tanh(action_means)
actions = tanh(actions)
action_logprobs = logprob_correction( action_logprobs)

return deterministic_actions, actions, action_logprobs # Also returns deterministic actions.
```

Also, note that the Stable Baselines also provides the ability to run SAC on pixel-based environments, namely by upgrading the MLP body of the policy with a CNN feature extractor.
That CNN feature extractor is also shared with 

**3. OpenAI SpinningUp**

```python
# Let the observation be defined by x, with a shape of [batch_size, *observation_shape]
# Define a classifcal MLP feed forward network as the body of the policy.
x = MLP(x)

# Additionally, we have two spearate fully connected (FC) layers that output the mean and the logstd of the action distribution, respectively
action_means = ActionMeanFC(x)
action_logstds = ActionLogstdFC(x)

# Clipping the logstd to the range [-20., 2.]
action_logstds = clamp(action_logstds, -20, 2) # Torch equivalent of clip_by_value.
action_stds = exp(action_logstds) # Apply exponetial to the logstd to recover the standard deviation itself,

# Instantiate the action distribution as a Gaussian parameterized by the previously obtained mean and standard deviations
action_dist = Normal( action_means, action_logstds)

# Samples some actions as well as their log probabilities
actions = action_dist.sample()
action_logprobs = action_dist.log_prob( actions)

# Squash the actions and scale them in case the action space is not [-1, 1.]
deterministic_actions = tanh(action_means)
actions = tanh(actions)
action_logprobs = logprob_correction( action_logprobs)

# Scale the actions to match the original action space
actions = action_limit * actions

return actions, action_logprobs # Also returns deterministic actions, depending on a flag passed when sampling the actions.
```

**4. Denis Yarats implementation**

Probably one of the early Pytorch implementation of the SAC, it follows the original implementation of the author (namely on how it output the joint action `means` and `logstd`).
Furthermore, instead of clipping the `logstd`, it uses the alternative method to cap them, which seems to have been proposed in OpenAI (as per [this comment](https://github.com/hill-a/stable-baselines/blob/3d115a3f1f5a755dc92b803a1f5499c24158fb21/stable_baselines/sac/policies.py#L210)).
Furthermore, it seems to be the only implementation which additional apply a `tanh` to the logstd before shifting them in the bounding interval, as well as different values for the bounding interval ([-5,2] instead of the "traditional" [-20,2]).

```python
# Let the observation be defined by x, with a shape of [batch_size, *observation_shape]
# Define a classifcal MLP feed forward network as the body of the policy.
x = MLP(x)

# Additionally, we a fully connected (FC) layers that outputs the mean and the logstd of the action distribution "jointly"
# Let Da be the dimension of the action space. MeanAndLogStd_FC has an output dimension of 2 * Da
means_and_logstds = MeanAndLogstd_FC(x)
action_means = means_and_logstds[:D] # The first half for the ation means, the latter for the logstds

action_logstds = means_and_logstds[D:] # The second half for the logstd
action_logstds = tanh(action_logstds)

# Let LOG_STD_MIN=-20, LOG_STD_MAX=2
action_logstds = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (action_logstds + 1)

action_stds = exp(action_logstds)

# Instantiate the action distribution as a Gaussian parameterized by the previously obtained mean and standard deviations
action_dist = Normal( action_means, action_logstds)
deterministic_actions = tanh(action_means)
actions = tanh(actions)
action_logprobs = logprob_correction( action_logprobs)l
# Note however that this part is done using Pytorch distributions combined with some "advanced" techniques:
# https://github.com/denisyarats/pytorch_sac/blob/815a77e48735416e4a25b44617516f3f89f637f6/agent/actor.py#L41

return actions, action_logprobs, determinsistic_actions
```

