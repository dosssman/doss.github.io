---
layout: single
title: "[WIP] Soft Actor Critic: Implementation and Experiments"
excerpt: "An attempt at expanding upon the theory and motivation behind the Soft Actor Critic algorithm for continuous and discrete action space, as well as their respective implemention details and variations, if any exist."
tags:
  - Reinforcement Learning
  - Implementation
  - Research
  - Policy Gradients
  - Entropy Maximization
  - Soft Actor Critic
toc: true
toc_sticky: true
author_profile: true
classes: wide
comments: true
---

# Introduction

The Soft Actor Critic (SAC) ([[1]](https://arxiv.org/abs/1702.08165), [[2]](https://arxiv.org/abs/1801.01290), [[3]](https://arxiv.org/abs/1803.06773), [[4]](https://arxiv.org/abs/1812.05905), [[5]](https://arxiv.org/abs/1910.07207)) built on top of the various progress made in Deep Reinforcement Learning (DRL) for continuous control.
It happens to address a few shortcomings the Deep Deterministic Policy Gradient (DDPG) ([[6]](https://arxiv.org/abs/1509.02971)) method.
Since a the SAC algorithm is deeply related with the DDPG one, being familiar with the latter should greatly facilitate understanding the SAC algorithm itself.
For the sake a completeness, a similar write-up of the DDPG algorithm and some of its intricacies can be found [here](/posts/2020-07-01-ddpg-experiments/).
Being familiar with the latter should thus greatly facilitate understanding the SAC algorithm, at least from our personal experience when trying to re-implement said algorithm.
{: .text-justify}

As an outline of this post, we first attempt at introducing the theory behind SAC as intuitively as possible.
Then, we go over the various implementation details of SAC that can be found across the various publicly available implementations, then introduce our own reimplementation, based on the work flow of the [CleanRL Reinforcement Learning library](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/sac_continuous_action.py).
Additional, we present some results of training an SAC agent over some popular DRL environments, as well as some additional experiments that we thought we thought would be interesting to play around.
{: .text-justify}

**TODO: Remove this additional experiment list once their are added at the bottom of the post**
First draft out all the experiments and their settings (environment, variable to check for the ablation, etc...)
then start queing them into the wandb repository.
Do it maybe for the simplest envs first: Hopper-v2 and HopperBulletEnv-v0 first, then expand to more envs latter.
- more critics
- delayed actor updates
- automatic entropy tuning and its relation with actor loss and such.
- training critics on different batches to achieve some sort of specialization.
- initialization schemes, layer normalization other network tweaks respective impact.
- impact of different activations in the intermediate layers

Without further ado, let us start with an overview before diving into the nitty-gritty details of the aforementioned algorithm.
{: .text-justify}

# Background

## Reinforcement Learning

<figure style="width: 320px" class="align-right">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/posts/ddpg_experiments/agentenvinteraction.png" alt="">
  <figcaption>Figure 1: Agent-environment interaction. Picture taken from <a href="http://incompleteideas.net/book/ebook/node28.html">Richard Sutton's book</a></figcaption>
</figure>
Reinforcement Learning (RL) is usually approached through the lenses of the Markov Decision Process (MDP), in the most straight-forward cases.
The MDP is defined as a five-tuple $$(S,A,P(s'\vert s, a),R(s,a),\gamma)$$, which respectively stand for the *state space*, *action space*, *transition dynamics*, *reward function* and *discount factor* (more [here](/posts/2020-07-01-ddpg-experiments/#the-mdp-framework)).
{: .text-justify}

The objective of a standard RL agent $$\pi$$ is to maximize the expected future reward, denoted here as $$J$$, which can be formally defined as:
{: .text-justify}

$$
J_\pi = \mathbb{E}_{s_t \sim P(\cdot \vert s_{t-1}, a_{t-1}), a_t \sim \pi(\cdot \vert s_t)} \left[ \sum_{t=0}^{\infty} \gamma ^ t r(s_t,a_t)\right] \quad (1)
$$

The agent is expected to achieve such goal by a trial-and-error process of interacting with the environment (Figure 1) and collecting experience (also referred to sampled trajectories, formally written as the tuple *(s,a,r,s')*).
{: .text-justify}

## Deterministic Policy Gradient

From personal experience, before challenging the SAC algorithm, it might be useful to get familliar with the simpler, but shared structure of offline policy gradients methods such as the [DDPG](posts/2020-07-01-ddpg-experiments/) ([OpenAI SpinningUp's DDPG](https://spinningup.openai.com/en/latest/algorithms/ddpg.html?source=post_page---------------------------)).
This is motivated by the observation that the key differences between DDPG and SAC would thus be (1) the stochastic policy used by the latter (which brings in the concepts of entropy and state-dependent exploration), and (2) having two Q-networks.

## Some of the theory behind the SAC

**1. Entropy Maximization**

As a direct consequence of having a stochastic policy (which we further explore in the next sub-section), on top of the environment reward the agent is supposed to maximize, an additional **entropy maximization** objective is added.
Intuitively, when we have a low entropy policy, the actions it outputs when in a given state are *consistent*, or in other word, have low variance.
For example, a deterministic policy such as in DDPG has no entropy, since for a given state, the output action shall always be the same.
{: .text-justify}

A moderate to high entropy policy, on the other hand, when given the same state, would instead output a different action every time we sample from it.
The higher the *distance* between the actions, the higher the entropy.
As described in the sub-section above, this a high entropy policy implicitly incorporates some *exploratory noise*, which is also somewhat dependent on the state itself.
{: .text-justify}

The entropy maximization objective is formally expressed by adding the entropy of the policy, weighted by as scalar in the original Equation (1), thus yielding the entropy maximization objective:
{: .text-justify}

$$
J_{\pi}^{\mathrm{MaxEnt}} = \mathbb{E}_{ (s_t,a_t) \sim \rho_{\pi}} \left[ \sum_{t=0}^{\infty} \gamma ^ t r(s_t,a_t) + \alpha \mathcal{H}\left[\pi(\cdot \vert s_t) \right] \right] \quad (2)
$$

where $$\rho_{\pi}$$ represets the state-actions transitions (or $$(s,a)$$ trajectories basically) that are induced by following said policy $$\pi$$.
{: .text-justify}

**2. Stochastic Policy**:

In contrast with the DDPG which is build around a parameterized deterministic policy, the SAC algorithm instead leverages a *parameterized stochastic policy*, which happens to be more expressive than its deterministic counterpart.
Indeed a stochastic policy first allows the agent to capture multiple near-optimal policies in the same structure.
An example of where this comes in handy could be described as follows: let us imagine the context of a agent controlling a car, that tries to go from city A to city B.
Now there might be multiple roads that lead from A to B, and since we want the agent to get there as fast as possible, we are interested in the short path.
It might happen that there exist two different roads that have the same, shortest length.
A deterministic agent would only be able to capture a single one of them (this is assuming it can find one of them), while a stochastic agent would be able to capture both of the roads.
While this is a really simplistic example, this is an invaluable property when we need the agent to be able to achieve some arbitrary goal in different ways.
{: .text-justify}

The second benefit of having a stochastic policy would be *state-depend exploration*.
Let us consider action that are sampled from a Normal distribution, which is parameterized by the *mean* and the *standard deviation*.
The *mean* action could be considered as the deterministic action the deterministic policy would take, such as in DDPG.
The *standard deviation* allows us to define a neighborhood around said the mean, such that actions that "are not necessarily the best" also get to be sampled.
{: .text-justify}

<figure class="one align-center">
    <a href="/assets/posts/sac/StateDependentExploration.svg"><img src="/assets/posts/sac/StateDependentExploration.svg"></a>
    <figcaption>Left: Uncorrelated and state independent exploration achieved by adding some randomly sampled noise to the actions (DDPG, TD3). Right: State dependent and correlated noise sampling, achieved by parameterizing both the mean and the standard deviation of a Normal distribution, from which the actions are sampled during during the training (SAC, PPO, TRPO).</figcaption>
</figure>

Therefore, by conditioning the *standard deviation* parameter of our Normal distribution, the agent can systematically control how much exploration (how far away from the "current" optimal action) to go in the action space.
As an intuitive example, the *standard deviation* of the agent for a state that it has encountered often enough would become lower as the agent converges to the optimal action.
On the other hand, the *standard deviation* of the agent for a state that might have not been explore well enough yet will be larger, thus making the agent behave less conservatively, and effectively exhausting the action space to build a better knowledge of that unexplored state.
{: .text-justify}

An additional benefit of having a *state-dependent* exploration is that it also results in a more smoothly correlated exploration process.
Indeed, when sampling noise from a normal distribution to add it to an action, for the same state, we would always get different and uncorrelated noise values (see [[8]](https://arxiv.org/abs/1706.01905)).
Parameterizing the "noise" of exploration by condition on the state instead results in more consistent exploratory behavior for an arbitrary state.
{: .text-justify}

For the sake of completeness, we now formally introduce the notation of the parameterized policy, as well as the derivation of the gradients of said parameters with regards the agent's objective function.
One might however skip this part at first if he is already familiar with the DRL definition, or it he does not mind just getting an more abstract picture of the algorithm.
{: .text-justify}

The stochastic policy of the agent is denoted as $$\pi_{\theta}(a \vert s)$$, which actually represents a Normal distribution with mean $\mu_{\theta}(s)$ and standard deviation $\sigma_{\theta}(s)$ such that:
{: .text-justify}

$$
a \sim \mathcal{N}( \mu_{\theta}(s), \sigma_{\theta}(s))
$$

Overloading $$J_{\pi}$$ with the entropy maximization objective introduced in Equation (2), we aim at training an agent that maximizes said $$J_{\pi}$$.
Formally, we write:
{: .text-justify}

$$
\theta^* = \mathrm{argmax}_{\theta} \enspace J_{\pi}
$$

The standard RL objective is optimized for by incrementally changing the parameters of the policy network so as *maximizing the probability of actions that contribute to higher reward, and minimizing the probability of actions that otherwise lead to low rewards*.
The entropy maximization objective. on the other hand, is optimized for by also updating the parameters of the policy network, but this time to keep its *entropy* as high as possible (note that said entropy is easily quantifiable for a Normal distribution).
Intuitively, this is equivalent to having a policy that takes relatively random actions given a state, but the variance of such actions is ultimately bounded to an interval that still allows the agent to achieve the original objective.
{: .text-justify}

A slightly more hands-on approach on the objective of the agent is presented [here](#policys-objective-and-optimization).

**3. Soft Q Value function**:

As a consequence of introducing the entropy maximization objective, the critic component of the standard actor-critic framework is also impacted.
Namely, we now use a *Soft Q value function* (originally presented back in [[1]](https://arxiv.org/abs/1702.08165)), which incorporates the entropy component of the policy while estimating the expected discounted future return.
{: .text-justify}

The author posit that the standard value function that evaluate a state-action pair and its corresponding entropy is an implicit policy.
Namely, by exponentiating the Q-value of all the actions given an arbitrary state, and regularizing it using the partition function over said state-action space, we can recover a probability distribution over the continuous action space.
An intuitive attempt at illustrating this concept is presented in the figure below.
{: .text-justify}

<figure class="one align-center">
    <a href="/assets/posts/sac/SoftQValueImplicitPolicy.svg"><img src="/assets/posts/sac/SoftQValueImplicitPolicy.svg"></a>
    <figcaption>Left: Soft Q Value for an arbitrary state s, and over all the possible continuous actions. Right: The implicit policy induced by the Soft Q Value function, which is recovered by exponentiating the Soft Q value of a state-action pair (numerator of the fraction), and regularized with the partition function (denominator of that same fraction). Intuitively, it is just a "softmax" operation performed over the whole state action space, with a focus on the actions.</figcaption>
</figure>


This implicit policy can then be used as a target for the actor's parameterized policy to optimize toward.
This process was formally defined as minimizing the KL divergence between the implicit policy recovered from the Soft Q Value and the actor's policy:
{: .text-justify}

Despite all this seemingly complicated process, the actual implementation differs from the DDPG algorithm by a really slight margin, since a lot of those *fancy* operation (exponentiation of the Q value, regularization with partition function and minimizing the KL divergence between the two policies) happen to cancel out when using the appropriate implementation technique.
{: .text-justify}

**4. Reducing Overestimation Bias with multiple critic functions**

This *implementation component* seems to have been introduced concurrently by not only the SAC algorithm, but also the Twin Delayed Deep Deterministic Policy Gradient (abbreviated as TD3 [[7]](https://arxiv.org/abs/1802.09477)).
(If we refer to the order in which the arXiv preprint were published, it might seem that the SAC [[2]](https://arxiv.org/abs/1801.01290) implements it first. However, the TD3 paper explicit investigates the problem of overestimation bias that occurs in DDPG, which suggests it is the origin of the technique of using multiple Q value function).
{: .text-justify}

Over-estimation bias can be understood as the Q function (or analog component) being overly optimistic, and consequently misrepresenting the landscape of the state-action space in which the agent's policy makes its decision.
Let us recall that in the policy gradient method, we aim at maximizing the likelihood of the actions that bring the agent the most reward, namely by following the guidance of a Q value function, for example.
But the Q value function itself is trained on trajectories data that are sampled with a sub-optimal policy.
If the latter does not happen to actually output actions that are close to the optimal ones for a given state, the Q value function is likely to attribute a higher value to those actions that are sub-optimal, to the detriment of the real, optimal actions.
Hence, we say that the Q-value function *overestimates the value of an action given an arbitrary state*.
{: .text-justify}

To mitigate this, the concept of using the minimum of at least 2 Q-values functions was introduced.
Intuitively, since the output of each respective Q-value function are bound to different, by taking the minimum of the estimated values of each action, we end up in a more conservative, and less biased approximation of its actual value.
{: .text-justify}

# Implementation details

This write-up compiles various implementations methods that appear across popular reinforcement learning libraries / repositories, as well as a few of the lesser known ones, namely:

- Original implementation by Tuomas Haarnoja et al. (Tensorflow v1.4) [Link](https://github.com/haarnoja/sac)
- Stable Baselines (Tensorflow 1.XX) [Link](https://github.com/hill-a/stable-baselines/tree/master/stable_baselines/sac)
- OpenAI Spinning Up (Tensorflow 1.XX, as well as PyTorch) [Link](https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/sac)
- Denis Yarats and Ilya Kostrikov's Implementation (Pytorch) [Link](https://github.com/denisyarats/pytorch_sac)

The component that happens to have the most variance in all those implementations happens to be the policy, which we go over in the next section.
{: .text-justify}

## Policy network

Given the implementations mentioned above not only use different deep learning frameworksd, but also adopt different coding style, we attempt to approach their respective implementations from a high-level (pseudo code) perspective, we a focus on the flow of the mapping from the observation received by the environment to the action distribution.
{: .text-justify}

First, an high-level overview of the various policy structure encountered is summarized in the graph below.
<figure class="one">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/posts/sac/policy_structures.png" alt="">
  <figcaption>Figure TODO: Policy network structures across publicly available implementations.</figcaption>
</figure>

A more detailed, while deep learning framework agnostic overview of those different implementations' respective pseudo-code can be found [further down below (Appendices)](#policy-network-detailed-pseudo-code), we provide a brief summary of the core difference between those implementations.
{: .text-justify}

**Policy network implementation differences**
- Either "Joint or disjoint parameterization" of the mean $\mu$ and log standard deviation $log \sigma$.
- Clipping or shifting the range of the $log \sigma$
- L2 regularization of the policy network's weights
- Squashing of the action with the $tanh$ function, and adjusting the $log\pi(a \vert s)$ correspondingly.

While the main difference in those implementation is the way the action distributions are parameterized (especially $\mathrm{log}\sigma$), their respective behaviors end up being similar, as per the following [interactive graph that modelizes the respecitve distributions](https://www.desmos.com/calculator/4sfqhlm50s).
{: .text-justify}

<figure class="one">
  <iframe src="https://www.desmos.com/calculator/hve72attpt?embed" width="100%" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe>
  <figcaption>Figure TODO: Intuitive representation of the resulting action distribution based on the policy network structure: in red, the parameterization scheme used by the original author's implementation and that of Yarats' et al; in blue the one used in OpenAI Stable Baselines and OpenAI SpinningUp </figcaption>
</figure>

## Hyper-parameters

Additionally, we have taken this opportunity to summarize the various hyper-parameter values used across the repositories referenced above for a more convenient comparison.

| Hyper parameter              |    Haarnoja et al.    |    Stable Baselines          | SpinningUp                   | Yarats et al.'s                   |
| :-------------               | :----------:          | :-----------:                | :-------------:              | :----------:                      |
| **General**                  |                       |  
| Discount $$\gamma$$          | 0.99                  | 0.99                         | 0.99                         | 0.99                              |
| Initial exploration (g)      | 1000 (a)              | 0 (`random_exploration`)     | 10000 (`start_steps`)        | 5000 (`num_seed_steps`)           |
| Tau (target update coef.)    | 0.005                 | 0.005                        | 0.005                        | 0.005                             |
| Reward scaling               | Yes (b)               | No                           | No                           | No                                |
| Buffer size                  | 1e6                   | 5e5                          | 1e6                          | 1e6                               |
| Batch size                   | 256                   | 64                           | 100                          | 1024                              |
| Seeding                      | {1,...,5}             | Random by default            | 0 by default                 | 1 by default                      |
| Entropy coefficient          | 0.0                   | Auto                         | 0.2                          | 0.1                               |
| Entropy auto-tuning          | No (c)               | Yes, see above               | No                           | Yes                               |
| Learning starts (g)          | 1000 (a)              | 100 (`learning_starts`)      | 1000 (`update_after`)        | 5000 (`num_seed_steps`)           |
| **Policy**                   |                       |
| Hidden layer dim.            | 256,256               | 64,64                        | 256,256                      | 1024,1024                         |
| `Logstd` range clip          | [-20,2] Clip (d)      | [-20,2] Clip Pass gradients  | [-20,2] Clip                 | tanh, then scale within [-5,2]    |
| Squashing                    | Yes                   | Yes                          | Yes                          | Yes                               |
| Update frequency             | 1                     | 1                            | 50 (f)                       | 1                                 |
| Learning rate                | 3e-4                  | 3e-4                         | 1e-3                         | 1e-4                              |
| Activation                   | ReLU (e)              | ReLU                         | ReLU                         | ReLU                              |
| Weight, Bias init.           | Xavier, Zeros         | Xavier, Zeros                | Torch nn.Linear's default    | Torch nn.Linear's default         |
| **Soft Q-Networks**          |                       |
| Hidden layer dim             | 256,256               | 64,64                        | 256, 256                     | 1024,1024                         |
| Learning rate                | 3e-4                  | 3e-4                         | 1e-3                         | 1e-4                              |
| Update frequency             | 1                     | 1                            | 50 (f)                       | 2                                 |
| Activation                   | ReLU (e)              | ReLU                         | ReLU                         | ReLU                              |
| Weight, Bias init.           | Xavier, Zeros         | Xavier, Zeros                | Torch nn.Linear's default    | Torch nn.Linear's default         |

Some additional notes regarding the hyper-parameters, or more general components of the implementations:

- (a) Some hyper parameters were set according to the specific environment, see [here](https://github.com/dosssman/sac/blob/master/examples/variants.py).
- (b) Also varies depending on the environment. The method of determination does not seem to be provided, however. Probably some "human priors": [Link](https://github.com/dosssman/sac/blob/master/examples/variants.py).
- (c) Added later in another [repository](https://github.com/rail-berkeley/softlearning), more specifically [here](https://github.com/rail-berkeley/softlearning/blob/46f14436f62465a02b99f431bbcf57a7fa0fd09d/softlearning/algorithms/sac.py#L253)
- (d) While some implementations ([Haarnoja et al.](https://github.com/haarnoja/sac), [Stable Baselines](https://github.com/hill-a/stable-baselines/tree/master/stable_baselines/sac), [Yarats et al](https://github.com/denisyarats/pytorch_sac), directly clip the `logstd` to the bounding interval (`[-20, 2]`), SpinningUp first applies `tanh` then scale it to the interval `[-20,2]`.
- (e) The source codes also includes the option to use the `tanh` non-linear activation, but the difference is result does not seem to be explored.
- (f) In OpenAI SpinningUP, the network updates are not done at every time step, but in an "epoch-based" fashion. Namely, a specific amount of transitions are collected, before updating the policy and critic network. In other words, the "sample-update" regime is different. There is not much difference in the empirical result, however.
- (g) The concept of `Initial exploration` and `Learning starts` can be found across various implementations, not limited to the SAC algorithm. The former is consistently used (DQN, DDPG, etc...) and determines how many steps are sampled using a uniform or random distribution over the action space, so as to fill the agent's experience with diverse, "non-biased" trajectories.
This is done to learn an accurate enough critic function at the beginning of the training, to provide a less bias, and thus better feedback to the policy once it starts to learn.
The `Learning Starts` hyper parameter is less common, and determines after how many steps we start updating the networks.
Intuitively, this is used to make sure we have enough data in the replay buffer before starting the updates.
From personal experience, this hyper-parameter hardly had mostly no impact on continuous control toy problems and Mujoco locomotions tasks.
- The original author's implementation, as well the one from OpenAI SpinUp evaluates the agent's performance using a deterministic policy.

Beside the implementation details touched upon above, the other aspects of the SAC algorithm implementation are the same across repository.
From here onward, we provide simplified implementation of the aforementioned components with the technique that worked best during our re-implementation phase.

# Simplified implementation

## Soft Q-Value function

The Soft Q-Value is likely the component with the less variance across the various reference implementation.
Similarly to DDPG [[6]](https://arxiv.org/abs/1509.02971), the Q value function simply maps from the concatenation of the current state and the action $s || a$ to a real value.
{: .text-justify}

Hereafter, a snippet of a simple implementation of such network using the Pytorch Deep Learning framework.
{: .text-justify}

```python
# observation_shape: the dimension of the observation vector that serves as input to the network
# action_shape: the dimension of the action vector

class SoftQNetwork(nn.Module):
    def __init__(self):
        super(SoftQNetwork, self).__init__()
        self.fc1 = nn.Linear(observation_shape+action_shape, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 1)

    def forward(self, x, a):
        x = torch.Tensor(x).to(device)
        x = torch.cat([x, a], 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

The same, but more contextualized implementation can be found in the SAC implementation of the [CleanRL library](https://github.com/vwxyzjn/cleanrl/blob/f4efe9ce4a0a98becbd02be7513fd1d49097500e/cleanrl/sac_continuous_action.py#L176).
{: .text-justify}

## Soft Q-Network objective and optimization

Slightly different from the traditional Q-network objective (DDPG, TD3, etc...), which predicts the "pure" expected discounted future return given a state, the **Soft Q-Network** incorporates the entropy of the policy into its objective, namely:
{: .text-justify}

$$
Q^{\pi}(s,a) = \mathbb{E}_{s' \sim P,a' \sim \pi} \big[ R(s,a) + \gamma (Q^\pi(s',a') + \alpha \underbrace{\mathcal{H}(\pi(\cdot \vert s'))}_{\text{policy's entropy}}) \big]
$$

(For a more in-depth definitions, see the [OpenAI SpinningUp more in-depth explanation](https://spinningup.openai.com/en/latest/algorithms/sac.html), and / or the [author's definition in Eq. 3](https://arxiv.org/abs/1812.05905))

The training objective of the Soft Q-Networks thus consists in reducing the _soft Bellman residual_ as follows:

$$
J_{Q}(\theta) = \frac{1}{2} \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim \mathcal{D}} \Big[ (Q_\theta(s_t, a_t) - y)^2 \Big] \\
\text{with} \enspace y = r(s_t, a_t) + \gamma (Q_\theta(s_{t+1},a_{t+1}) - \alpha \text{log} \pi_{\phi}(a_{t+1} \vert s_{t+1})), \\
\text{and where} \enspace a_{t+1} \sim \pi_\phi(\cdot \vert s_{t+1}),
$$

where $(s_t,a_t,r_t,s_{t+1} \sim \mathcal{D})$ represent a batch of state, action, reward and next_state sample from the agent's experience (trajectories collected by having the agent's policy $\pi_{\phi}$ interact with the environment).
{: .text-justify}

The mitigation of the over-estimation bias argued for in the original paper is thus achieved by having two (or more) independently parameterized soft Q-networks.
Be it in the SAC, or the TD3 algorithm, the number of Q-network required to do so was empirically determined to be "at least 2"
{: .text-justify}

In any case, letting our two soft Q-networks be $Q_{\theta_1}$ and $Q_{\theta_2}$ respectively parameterized by $\theta_1$ and $\theta_2$, the more explicit optimization objective can be written as follows:
{: .text-justify}

$$
J_{Q}(\theta_{i}) = \frac{1}{2} \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim \mathcal{D}} \Big[ (Q_{\theta_i}(s_t, a_t) - y)^2 \Big] \\
\text{with} \enspace y = r(s_t, a_t) + \gamma ({\color{red} \min_{\theta_{1,2}}Q_{\theta_i}(s_{t+1},a_{t+1})} - \alpha \text{log} \pi_{\phi}(a_{t+1} \vert s_{t+1}))
$$

Practically speaking, this whole optimization of the Soft Q-Networks is achieved by the following snippet of code:

```python
s_obs, s_actions, s_rewards, s_next_obses, s_dones = rb.sample(args.batch_size) # sample data from agent's experience

with torch.no_grad():
    next_state_actions, next_state_log_pi, _ = pg.get_action(s_next_obses, device) # get next action a_{t+1} and log pi(a_{t+1} | s_{t+1})
    qf1_next_target = qf1_target.forward(s_next_obses, next_state_actions, device) # get Q_1(s_{t+1}, a_{t+1})
    qf2_next_target = qf2_target.forward(s_next_obses, next_state_actions, device)
    min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi # add the minimum of the Q values and add the policy entropy bonus
    next_q_value = torch.Tensor(s_rewards).to(device) + (1 - torch.Tensor(s_dones).to(device)) * args.gamma * (min_qf_next_target).view(-1) # target update (here the s_dones allow to adjust for when episodes end)

qf1_a_values = qf1.forward(s_obs, torch.Tensor(s_actions).to(device), device).view(-1) # get Q_1(s_t, a_t)
qf2_a_values = qf2.forward(s_obs, torch.Tensor(s_actions).to(device), device).view(-1)
qf1_loss = loss_fn(qf1_a_values, next_q_value) # Compute the TD error with respect to y(s,a) for the first Q network, and so on
qf2_loss = loss_fn(qf2_a_values, next_q_value)
qf_loss = (qf1_loss + qf2_loss) / 2

# here, values_optimizer is an Adam (torch.optim.Adam) optimizer that handles the weight of both Q networks
values_optimizer.zero_grad()
qf_loss.backward()
values_optimizer.step()
```

For the more contextualized source code, please refer to [this link](https://github.com/vwxyzjn/cleanrl/blob/af2344d3e13c5ab993075f30b5e1fdeb7aa0da38/cleanrl/sac_continuous_action.py#L258).

**TODO**: In our ablation study on the impact of the number of Q function can be found [insert link here]. Indeed as long as there is at least two Q-networks, there is a significant improvement in the quality of the Q estimates (namely, less overestimation).
Having more than 2 networks does not really improved the performance overall. (From our observation, 3 Q function actually resulted in stabler improvements in some tasks, but the experimental pool of environment is not wide enough to make any conclusions I would say.)
**TODO**: Reroute to the corresponding section below
{: .text-justify}

## Policy network

Finally, we present a snippet of the implementation used, which uses the "disjoint parameterization" method, but the `tanh` the scale processing applying to the `logstd` component.
{: .text-justify}

```python
# observation_shape: the dimension of the observation vector that serves as input to the network
# action_shape: the dimension of the action vector
# LOG_STD_MAX = 2
# LOG_STD_MIN = -5

class Policy(nn.Module):
    def __init__(self):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(observation_shape, 256) # Better result with slightly wider networks.
        self.fc2 = nn.Linear(256, 128)
        self.mean = nn.Linear(128, action_shape)
        self.logstd = nn.Linear(128, action_shape)
        # action rescaling
        self.action_scale = torch.FloatTensor(
            (env.action_space.high - env.action_space.low) / 2.)
        self.action_bias = torch.FloatTensor(
            (env.action_space.high + env.action_space.low) / 2.)
        self.apply(layer_init)

    def forward(self, x):
        x = torch.Tensor(x).to(device)

        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        mean = self.mean(x)
        log_std = self.logstd(x)
        # From Denis Yarats' implementation: scaling the logstd with tanh,
        # and bounding it to a reasonable interval
        log_std = torch.tanh(log_std)
        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1) 

        return mean, log_std

    def get_action(self, x):
        mean, log_std = self.forward(x)
        std = log_std.exp()
        normal = Normal(mean, std)
        x_t = normal.rsample()  # reparameterization trick (mean + std * N(0,1))
        y_t = torch.tanh(x_t)
        action = y_t * self.action_scale + self.action_bias
        log_prob = normal.log_prob(x_t)
        # Enforcing Action Bound with tanh squashing
        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) +  1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        mean = torch.tanh(mean) * self.action_scale + self.action_bias

        return action, log_prob, mean

    def to(self, device):
        self.action_scale = self.action_scale.to(device)
        self.action_bias = self.action_bias.to(device)
        return super(Policy, self).to(device)
```

Similarly to the Q-network, a more contextualized implementation can be found [here](https://github.com/vwxyzjn/cleanrl/blob/f4efe9ce4a0a98becbd02be7513fd1d49097500e/cleanrl/sac_continuous_action.py#L131).
{: .text-justify}

## Policy's objective and optimization

Recall the maximum entropy objective from Eq. 2 our agent is expected to optimize toward.
In practice, with a parameterized Gaussian policy $\pi_\phi$ as described above, the objective becomes:
{: .text-justify}

$$
\text{max}_{\phi} \, J_{\pi}(\phi) = \mathbb{E}_{s \sim \mathcal{D}} \Big[ \text{min}_{j=1,2} Q_{\theta_j}( s, a) - \alpha \text{log}\pi_{\phi}(a \vert s) \Big] \\
\text{where } a = \mu_{\phi}(s) + \epsilon \, \sigma_{\phi}(s), \text{ with } \epsilon \sim \mathcal{N}(0, 1) \text{ (reparameterization trick.)}
$$

Intuitively, we optimize the weights $\phi$ of the policy network so that it (1) outputs actions $a$ that correspond to a high Q-value $Q(s,a)$, while at the same time (2) making sure that the distribution over those actions $a$ is random enough (maximum entropy).
{: .text-justify}

[This part is quite straightforward to implement, as per the follow code snippet.](https://github.com/vwxyzjn/cleanrl/blob/af2344d3e13c5ab993075f30b5e1fdeb7aa0da38/cleanrl/sac_continuous_action.py#L279)
```python
s_obs, s_actions, s_rewards, s_next_obses, s_dones = rb.sample(args.batch_size) # sample data from agent's experience

pi, log_pi, _ = pg.get_action(s_obs, device) # get action and their log probability
qf1_pi = qf1.forward(s_obs, pi, device) # get Q-value for state action pairs
qf2_pi = qf2.forward(s_obs, pi, device)
min_qf_pi = torch.min(qf1_pi, qf2_pi).view(-1)

policy_loss = ((alpha * log_pi) - min_qf_pi).mean() # maximiizes Q(s,a) and -log \pi(a|s), as per Eq. above

# apply one gradient update step
policy_optimizer.zero_grad()
policy_loss.backward()
policy_optimizer.step()
```

**TODO**: mention the detach() that breaks the learning, and why we use two optimizers.
Propose the trick to use same optimizer for both network by setting q_networks to .eval().

## Automatic Entropy tuning
In the objective of the policy defined above, the coefficient of the _entropy bonus_ $\alpha$ is kept fixed all across the training.
As suggested by the authors in Section 5 of their _Soft Actor Critic And Applications_([[4]](https://arxiv.org/abs/1812.05905)), the original purpose of augmenting the standard reward with the entropy of the policy is to *encourage explorations* of yet well enough explored state (thus high entropy).
Conversely, for states where the policy has already learned a near-optimal policy, it would be preferable to reduce the entropy bonus of the policy, so that it does not _lose its way by exploring, due to being encouraged to have high entropy_.
{: .text-justify}

Therefore, having a fixed value for $\alpha$ does not fit this desiderata of matching the entropy bonus with the knowledge of the policy at an arbitrary state during its training.
{: .text-justify}

To mitigate this, the authors proposed a method to dynamically adjust $\alpha$ as the policy is trained.
While the theoretical method that is used is skipped for the sake of brevity, the gist would be that they first define an optimization constraint over the reward, subject to a lower-bound on the entropy of the policy (the bonus).
The optimal value for $\alpha$ happens to be the dual of the optimization problem introduced earlier, which objective is defined as follow:

$$
\alpha^{*}_t = \text{argmin}_{\alpha_t} \mathbb{E}_{a_t \sim \pi^{*}_t} \big[ -\alpha_t \, \text{log}\pi^{*}_t(a_t \vert s_t; \alpha_t) - \alpha_t \mathcal{H} \big],
$$

where $$\mathcal(H)$$ represents the _target entropy_, or in other words, the desired lower-bound for the expected entropy of the policy over the trajectory distribution $(s_t, a_t) \sim \rho_\pi$ induced by the latter.
As a heuristic for the _target entropy_, the authors use dimension of the action space of the task.
{: .text-justify}

Once the optimization problem is formulated this way, the implementation becomes relatively straightforward:
{: .text-justify}
```python
## Before the training actually begins
# defined an initial value for (log) alpha, 
# as well as thecorreponding optimizer (Adam)
target_entropy = - torch.prod(torch.Tensor(env.action_space.shape).to(device)).item()
# instead of alpha, uses log_alpha for numerical stability
log_alpha = torch.zeros(1, requires_grad=True, device=device)
alpha = log_alpha.exp().item() # the actual value of alpha
a_optimizer = optim.Adam([log_alpha], lr=1e-3)
```

With the optimizer declared above, the `log_alpha` is then updated jointly with the policy as follows, so as to tweak the value of $\alpha$ based on the current entropy of the policy, as per the equation above.
{: .text-justify}

```python
with th.no_grad():
    _, resampled_logprobs = policy.get_actions(observation_batch)
alpha_loss = (-log_alpha * (resampled_logprobs + target_entropy)).mean()

a_optimizer.zero_grad()
alpha_loss.backward()
a_optimizer.step()

alpha = log_alpha.exp().item()
```

# Experiments

- Toy problems. Especially mountain car continuous will provide some discussion regarding the automatic entropy tuning and such
- Pybullet and Mujoco envs. Do we get similar results to publicly available benchmarks
- Discrete toy environments

# Conclusion

- Aims at providing a step by step guide to implement SAC, modular and easy to edit. Also plug in CleanRL link
- Added a few other experiments to help improve some performance. While most do not really matter, at least you know it know.

# Acknowledgment

- OpenAI Spinning Up , Harnooja, Kostrykov and Denis Yarats' respective implementations.
- Costa Huang and the Clean RL group **TODO: Insert plug** for insight sharing during implementation and discussions.

# Appendices

## Policy network detailed pseudo-code

**TODO: reduce the comments, make them 80 columns so that sliders don't appear on full screen at least.**

**1. Original implementation by Tuomas Haarnoja**

```python
# Let the observation be defined by x, with a shape of [batch_size, *observation_shape]
# Define a classifcal MLP feed forward network as the body of the policy.
x = MLP(x)

# Additionally, we a fully connected (FC) layers that outputs the mean and the logstd of the action distribution "jointly"
# Let Da be the dimension of the action space. MeanAndLogStd_FC has an output dimension of 2 * Da
means_and_logstds = MeanAndLogstd_FC(x)
action_means = means_and_logstds[:D] # The first half for the ation means, the latter for the logstds

action_logstds = means_and_logstds[D:] # The second half for the logstd
# Contrary to other implementation which separate the fully connected layers of the means and logstd, the original implementation outputs them jointly.
# There does not seem to be much difference in the results, however.

# Clipping the logstd to the range [-20., 2.]
action_logstds = clip_by_value(action_logstds, -20, 2) # This unfortunately will not produce gradients IF the logstd are outside of that range
action_stds = exp(action_logstds) # Apply exponetial to the logstd to recover the standard deviation itself,

# Instantiante the action distribution as a Gaussian parameterized by the previously obtained mean and standard deviations
action_dist = Normal( action_means, action_logstds)

# Samples some actions as well as their log probabilities
actions = action_dist.sample()
action_logprobs = action_dist.log_prob( actions)

# The actions are squashed to the [-1, 1] range by defualt with the tanh function.
# It however requires some correction to the action log probabiliities too
actions = tanh(actions)
action_logprobs = logprob_correction( action_logprobs)

# L2 regularization for the policy's weights. https://github.com/haarnoja/sac/blob/8258e33633c7e37833cc39315891e77adfbe14b2/sac/distributions/normal.py#L69

return actions, action_logprobs # The implementation actually returns determinsitcs actions (the action_means) that are used for the agent's evaluation.
```

**2. Stable Baselines**

This implementation uses a lot of "low-level" techniques to implement the various operations need to compute the action and their log probabilities
(https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/sac/policies.py).
But the concept is the same as in the original implementation.
{: .text-justify}

The main difference with the original implementation is that the gradient clipping uses a special method denoted as `clip_but_pass_gradient` instead of the default one provided by Tensorflow (or Pytorch).
It's implementations is as follows:
{: .text-justify}

```python
def clip_but_pass_gradient(input_, lower=-1., upper=1.):
    clip_up = tf.cast(input_ > upper, tf.float32)
    clip_low = tf.cast(input_ < lower, tf.float32)
    return input_ + tf.stop_gradient((upper - input_) * clip_up + (lower - input_) * clip_low)
```
Intuitively, since we have to recover the `action_stds` by exponentiating the `action_logstds`, some values of the latter can cause either gradient explosion (NaN, when logstd is too big) or gradient vanishing (namely when logstds goes to the negative infinity).
Thus clipping the values of `action_logstds` stabilizes the learning process and gives us a "nice" distribution over the actions.
Additionally, it constrains the action distribution to a reasonable standard deviation, so that we do not end up with a random policy (high standard deviations).
{: .text-justify}

The standard clamping method, however, does not pass any gradient when `logstd` happens to be outside of the "clamping range".
Therefore, there is not much feedback for the weights of the network to learn from.
The `clip_but_pass_gradient` thus seems to be used to circumvent such constraint.
{: .text-justify}

```python
# Let the observation be defined by x, with a shape of [batch_size, *observation_shape]
# Define a classifcal MLP feed forward network as the body of the policy.
x = MLP(x)

# Additionally, we have two spearate fully connected (FC) layers that output the mean and the logstd of the action distribution, respectively
action_means = ActionMeanFC(x)
action_logstds = ActionLogstdFC(x)

# Clipping the logstd to the range [-20., 2.]
action_logstds = clip_but_pass_gradient(action_logstds, -20, 2)
action_stds = exp(action_logstds) # Apply exponetial to the logstd to recover the standard deviation itself,

# Instantiate the action distribution as a Gaussian parameterized by the previously obtained mean and standard deviations
action_dist = Normal( action_means, action_logstds)

# Samples some actions as well as their log probabilities
actions = action_dist.sample()
action_logprobs = action_dist.log_prob( actions)

# At the time of writting, stable baselines did not seem to implement the L2 regularization for the means and logstd: https://github.com/hill-a/stable-baselines/blob/3d115a3f1f5a755dc92b803a1f5499c24158fb21/stable_baselines/sac/policies.py#L219

# Squash the actions to range [-1, 1] and correct the log_probs accordingly
# https://github.com/hill-a/stable-baselines/blob/3d115a3f1f5a755dc92b803a1f5499c24158fb21/stable_baselines/sac/policies.py#L44
deterministic_actions = tanh(action_means)
actions = tanh(actions)
action_logprobs = logprob_correction( action_logprobs)

return deterministic_actions, actions, action_logprobs # Also returns deterministic actions.
```

Also, note that the Stable Baselines also provides the ability to run SAC on pixel-based environments, namely by upgrading the MLP body of the policy with a CNN feature extractor.
That CNN feature extractor is also shared with 

**3. OpenAI SpinningUp**

```python
# Let the observation be defined by x, with a shape of [batch_size, *observation_shape]
# Define a classifcal MLP feed forward network as the body of the policy.
x = MLP(x)

# Additionally, we have two spearate fully connected (FC) layers that output the mean and the logstd of the action distribution, respectively
action_means = ActionMeanFC(x)
action_logstds = ActionLogstdFC(x)

# Clipping the logstd to the range [-20., 2.]
action_logstds = clamp(action_logstds, -20, 2) # Torch equivalent of clip_by_value.
action_stds = exp(action_logstds) # Apply exponetial to the logstd to recover the standard deviation itself,

# Instantiate the action distribution as a Gaussian parameterized by the previously obtained mean and standard deviations
action_dist = Normal( action_means, action_logstds)

# Samples some actions as well as their log probabilities
actions = action_dist.sample()
action_logprobs = action_dist.log_prob( actions)

# Squash the actions and scale them in case the action space is not [-1, 1.]
deterministic_actions = tanh(action_means)
actions = tanh(actions)
action_logprobs = logprob_correction( action_logprobs)

# Scale the actions to match the original action space
actions = action_limit * actions

return actions, action_logprobs # Also returns deterministic actions, depending on a flag passed when sampling the actions.
```

**4. Denis Yarats implementation**

Probably one of the early Pytorch implementation of the SAC, it follows the original implementation of the author (namely on how it output the joint action `means` and `logstd`).
Furthermore, instead of clipping the `logstd`, it uses the alternative method to cap them, which seems to have been proposed in OpenAI (as per [this comment](https://github.com/hill-a/stable-baselines/blob/3d115a3f1f5a755dc92b803a1f5499c24158fb21/stable_baselines/sac/policies.py#L210)).
Furthermore, it seems to be the only implementation which additional apply a `tanh` to the logstd before shifting them in the bounding interval, as well as different values for the bounding interval ([-5,2] instead of the "traditional" [-20,2]).

```python
# Let the observation be defined by x, with a shape of [batch_size, *observation_shape]
# Define a classifcal MLP feed forward network as the body of the policy.
x = MLP(x)

# Additionally, we a fully connected (FC) layers that outputs the mean and the logstd of the action distribution "jointly"
# Let Da be the dimension of the action space. MeanAndLogStd_FC has an output dimension of 2 * Da
means_and_logstds = MeanAndLogstd_FC(x)
action_means = means_and_logstds[:D] # The first half for the ation means, the latter for the logstds

action_logstds = means_and_logstds[D:] # The second half for the logstd
action_logstds = tanh(action_logstds)

# Let LOG_STD_MIN=-20, LOG_STD_MAX=2
action_logstds = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (action_logstds + 1)

action_stds = exp(action_logstds)

# Instantiate the action distribution as a Gaussian parameterized by the previously obtained mean and standard deviations
action_dist = Normal( action_means, action_logstds)
deterministic_actions = tanh(action_means)
actions = tanh(actions)
action_logprobs = logprob_correction( action_logprobs)l
# Note however that this part is done using Pytorch distributions combined with some "advanced" techniques:
# https://github.com/denisyarats/pytorch_sac/blob/815a77e48735416e4a25b44617516f3f89f637f6/agent/actor.py#L41

return actions, action_logprobs, determinsistic_actions
```
