---
layout: single
title: "[WIP] Soft Actor Critic: Implementation and Experiments"
excerpt: "An attempt at expanding upon the theory and motivation behind the Soft Actor Critic algorithm for continuous and discrete action space, as well as their respective implemention details and variations, if any exist."
tags:
  - Reinforcement Learning
  - Implementation
  - Research
  - Policy Gradients
  - Entropy Maximization
  - Soft Actor Critic
toc: true
toc_sticky: true
author_profile: true
classes: wide
comments: true
---

# Introduction

The Soft Actor Critic (SAC) ([[1]](https://arxiv.org/abs/1702.08165), [[2]](https://arxiv.org/abs/1801.01290), [[3]](https://arxiv.org/abs/1803.06773), [[4]](https://arxiv.org/abs/1812.05905), [[5]](https://arxiv.org/abs/1910.07207)) built on top of the various progress made in Deep Reinforcement Learning (DRL) for continuous control.
It happens to address a few shortcomings the Deep Deterministic Policy Gradient (DDPG) ([[6]](https://arxiv.org/abs/1509.02971)) method.
Since a the SAC algorithm is deeply related with the DDPG one, being familiar with the latter should greatly facilitate understanding the SAC algorithm itself.
For the sake a completness, a similar write-up of the DDPG algorithm and some of its intricacies can be found [here](/posts/2020-07-01-ddpg-experiments/).
Being familiar with the latter should thus greatly facilitate understanding the SAC algorithm, at least from our personal experience when trying to re-implement said algorithm.

As an outline of this post, we first attempt at introducing the theory behind SAC as intuitively as possible.
Then, we go over the various implementation details of SAC that can be found across the various publicly available implementations, then introduce our own reimplementation, based on the work flow of the [CleanRL Reinforcement Learning library](https://github.com/vwxyzjn/cleanrl).
Additional, we present some results of training an SAC agent over some popular DRL environments, as well as some additional experiments that we thought we thought would be interesting to play around.

**TODO: Remove this additional experiment list once their are added at the bottom of the post**
- weight sharing between the critics
- more critics
- delayed actor updates
- critic updated over different batches
- automatic entropy tuning and its relation with actor loss and such.
- training critics on different batches to achieve some sort of specialization.
- discrete SAC
- initialization schemes, layer normalization other network tweaks respective impact.

Without further ado, let us start with an overview before diving into the nitty-gritty details of the aforementioned algorithm.
{: .text-justify}

# Background

## Reinforcement Learning

## Deterministic Policy Gradient
- A quick refresh of DDPG or just a direct link even.

## Theory behind the SAC algorithm

**1. Soft Q Value function**: The first improvement of the SAC (presented back in [[1]](https://arxiv.org/abs/1702.08165)), was the introduction of the *Soft Q Value function*.
The Soft Q Value function is a transformation of the standard Q Value function into a policy by ... **TODO: Is this necessary ? Can we find a more intuitive explanation ?**.
{: .text-justify}

**2. Stochastic Policy**:
In contrast with the DDPG which is build around a parameterized deterministic policy, the SAC algorithm instead leverages a *parameterized stochastic policy*, which happens to be more expressive than its deterministic counterpart.
Indeed a stochastic policy first allows the agent to capture multiple near-optimal policies in the same structure.
An example of where this comes in handy would be as follows: let us imagine the context of a agent controlling a car, that tries to go from city A to city B.
Now there might be multiple roads that lead from A to B, and since we want the agent to get there as fast as possible, we are interested in the short path.
It might happen that there exist two different roads that have the same, shortest length.
A deterministic agent would only be able to capture a single one of them (this is assuming it can find one of them), while a stochastic agent would be able to capture both of the roads.
While this is a really simplistic example, this is an invaluable property when we need the agent to be able to achieve some arbitrary goal in different ways.
{: .text-justify}

The second benefit of having a stochastic policy would be *state-depend exploration*.
More specifically, with a deterministic policy such as in DDPG, the agent is made to explore the state-action space by adding some noise to the actions it outputs, noise which is sample from an external noise process, such as a Gaussion distribution for example.
**TODO: Improve this way to janky presentation of stochastic policy**.
With a stochastic policy, however, since the action is sampled from a parameterized Gaussian distribution for example.
{: .text-justify}

**3. Entropy Maximization**
This is actually a direct consequence of having a stochastic policy, and goes hand to hand with the exploration aspect of the agent, and its ability to capture multiple optimal policies.
{: .text-justify}

**4. Reducing Overestimation Bias with multiple critic functions**
This *implementation trick* seems to have been introduced concurrently by not only the SAC algorithm, but also the Twin Delayed Deep Deterministic Policy Gradient (abbreviated as TD3 [[7]](https://arxiv.org/abs/1802.09477)).
(If we refer to the order in which the arXiv preprint were published, it seems that the SAC [[2]](https://arxiv.org/abs/1801.01290) implements it first).
{: .text-justify}

Over-estimation bias can be understood as the Q function (or analog component) being overly optimistic, and consequently misrepresenting the landscape of the state-action space in which the agent's policy makes its decision.
Let us recall that in the policy gradient method, we aim at maximizing the likelihood of the actions that bring the agent the most reward, namely by following the guidance of a Q value function, for example.
But the Q value function itself is trained on trajectories data that are sampled with a sub-optimal policy.
If the latter does not happen to actually output actions that are close to the optimal ones for a given state, the Q value function is likely to attribute a higher value to those actions that are sub-optimal, to the detriment of the real, optimal actions.
Hence, we say that the Q-value function *overestimates the value of an action given an arbitrary state*.

To mitigate this, the concept of using the minimum of at least 2 Q-values functions was introduced.
Intuitively, since the output of each respective Q-value function are bound to different, by taking the minimum of the estimated values of each action, we end up in a more conservative, and less biased approximation of its actual value.

## Soft Q Value function
- Introducing the q function parameterized network for continous action case.
- Mention that we actually use 2 of them, thus defined the two different \theta.
- Introducte the objective function, with the parameterized update equations
- **TODO** Mention the version with more q function their respective impact on the overall performance. Does it change anything ? If not, let us skip this part then.

## Stochastic Policy Gradient
- Introduce the stochastic policy as a parameterized neural network as well as its objective function

## Maximum entropy

- Entropy as a measure of uncertainty: recall the entropy definition of a Normal distrib and bridge it into the action of a policy
- Recall the stochastic policy introduced above, and reason that a low entropy policy is close to a determistic one, since only the actions around the mean will be sampled most of the time (sharp dist around the mean)
- A high entropy policy however, is close to a random one, in that action that are far from the mean of the distribution can also be sampled
- SAC aims retaining some degree of high entropy in the policy, which helps with exploration for example, and also allows to encode multiple near-optimal policies.

# Implementation details

## Multiple Q-Networks

- Some do 2 separate networks, some do it as a single network that returns two different values. In any case, the concept is the same.
- No weight sharing so far, but it might be interesting to see how it helps.

## Policy network

- Various ways of implementing the stochastic policy. (checkout Hafners Desmos post to illustrate the behavior of the parameterized distribution in SpinUp etc..)

## Training loop and logic

# Experiments

- Toy problems. Especially mountain car continuos will provide some discussion regarding the automatic entropy tuning and such
- Pybullet and Mujoco envs. Do we get similar results to publicly available benchmarks
- Discrete toy environments

# Conclusion

- Aims at providing a step by step guide to implement SAC, modular and easy to edit. Also plug in CleanRL link
- Added a few other experiments to help improve some performance. While most do not really matter, at least you know it know.

# Acknowledgment
- OpenAI Spinning Up , Harnooja, Kostrykov and Denis Yarats' respectivev implementations.
- Costa Huang and the Clean RL group **TODO: Insert plug** for collaboration during implementations and discussions
